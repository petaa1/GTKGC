{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "def preprocess_and_tokenize(file_path):\n",
    "    \"\"\"\n",
    "    Reads the file, preprocesses it, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    input_texts = []\n",
    "    target_labels = []\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Non-empty line\n",
    "            token, label = line.split()\n",
    "            current_tokens.append(token)\n",
    "            current_labels.append(label)\n",
    "        else:  # Empty line signifies end of a sentence\n",
    "            if current_tokens:\n",
    "                input_texts.append(' '.join(current_tokens))\n",
    "                target_labels.append(' '.join(current_labels))\n",
    "                # Reset for next sentence\n",
    "                current_tokens = []\n",
    "                current_labels = []\n",
    "\n",
    "    # Add the last sentence if the data doesn't end with an empty line\n",
    "    if current_tokens:\n",
    "        input_texts.append(' '.join(current_tokens))\n",
    "        target_labels.append(' '.join(current_labels))\n",
    "    \n",
    "    return input_texts, target_labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = 'NER/Training_Data/AutoLabelledSet.txt'  # Replace with your file path\n",
    "input_texts, target_labels = preprocess_and_tokenize(file_path)\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25554\n",
      "Number of testing samples: 6389\n",
      "['4.2 local geology the project is underlain entirely by rocks of the coolgardie domain , a sub domain of the kalgoorlie terrane .', 'the abundant crystals or phenocrysts are totally altered feldspars and biotite plus the quartz .', 'recent work by blina resources nl has announced the discovery of numerous diamond bearing palaeochannels on their tenements held in joint venture with the kimberley diamond company immediately to the east of nwds kimberley downs prospects .', 'exploration comprised prospecting , rock sampling and stream sediment geochemistry .', 'historical drilling and surface sampling gold deposit legend historical drilling historical surface sampling cheroona well project 2006 annual report page 26 of 35 5.0 previous work gleneagle gold limited during the previous reporting period from 20th july 2005 to 31st march 2006 , exploration conducted by gleneagle gold limited included the processing of geophysical datasets , investigation and compilation of wamex open file data and the collection of two rock chip samples .']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    input_texts, target_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of testing samples: {len(test_texts)}\")\n",
    "\n",
    "print(train_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input IDs: tensor([[    3, 19765,   415,  ...,     0,     0,     0],\n",
      "        [    8, 16346,  6884,  ...,     0,     0,     0],\n",
      "        [ 1100,   161,    57,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   48,  3303,    65,  ...,     0,     0,     0],\n",
      "        [    8, 11508,     9,  ...,     0,     0,     0],\n",
      "        [ 2724,  4012,  2155,  ...,     0,     0,     0]])\n",
      "Training Labels: tensor([[411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 272,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 272,  18,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0]])\n",
      "Testing Input IDs: tensor([[   12,     8,  3457,  ...,     0,     0,     0],\n",
      "        [    3,     9,  4727,  ...,     0,     0,     0],\n",
      "        [ 8282,  1467,    13,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    8,  3731,   159,  ...,     0,     0,     0],\n",
      "        [ 1877,  2122,  2252,  ...,     0,     0,     0],\n",
      "        [18771,    19, 17509,  ...,     0,     0,     0]])\n",
      "Testing Labels: tensor([[411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/t5-base-conll03-english\")\n",
    "\n",
    "def tokenize_data(texts, labels):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and labels.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    targets = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure labels are the same length as inputs\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Tokenize training and testing data\n",
    "train_data = tokenize_data(train_texts, train_labels)\n",
    "test_data = tokenize_data(test_texts, test_labels)\n",
    "\n",
    "# Print tokenized data for verification\n",
    "print(\"Training Input IDs:\", train_data['input_ids'])\n",
    "print(\"Training Labels:\", train_data['labels'])\n",
    "print(\"Testing Input IDs:\", test_data['input_ids'])\n",
    "print(\"Testing Labels:\", test_data['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = NERDataset(train_data, train_data['labels'])\n",
    "test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=12, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"dbmdz/t5-base-conll03-english\").to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=10000,\n",
    "    eval_steps=10000,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Define metrics computation\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Flatten the lists\n",
    "    labels = labels.flatten()\n",
    "    predictions = predictions.flatten()\n",
    "    return {\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646d4d4be040420e85126fe6ac838fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.168, 'grad_norm': 0.06683902442455292, 'learning_rate': 4.93734335839599e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0681, 'grad_norm': 0.058328814804553986, 'learning_rate': 4.8746867167919805e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0586, 'grad_norm': 0.10578988492488861, 'learning_rate': 4.81203007518797e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0527, 'grad_norm': 0.08469075709581375, 'learning_rate': 4.74937343358396e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0463, 'grad_norm': 0.2472144067287445, 'learning_rate': 4.6867167919799495e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0388, 'grad_norm': 0.29074814915657043, 'learning_rate': 4.62406015037594e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0361, 'grad_norm': 0.2826578915119171, 'learning_rate': 4.56140350877193e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0314, 'grad_norm': 0.16509296000003815, 'learning_rate': 4.49874686716792e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0283, 'grad_norm': 0.17966510355472565, 'learning_rate': 4.43609022556391e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0272, 'grad_norm': 0.18065638840198517, 'learning_rate': 4.3734335839599e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0256, 'grad_norm': 0.16798895597457886, 'learning_rate': 4.3107769423558896e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0243, 'grad_norm': 0.11661367118358612, 'learning_rate': 4.24812030075188e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0226, 'grad_norm': 0.30791494250297546, 'learning_rate': 4.18546365914787e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0221, 'grad_norm': 0.15503369271755219, 'learning_rate': 4.12280701754386e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0208, 'grad_norm': 0.12246572971343994, 'learning_rate': 4.0601503759398494e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0205, 'grad_norm': 0.43900102376937866, 'learning_rate': 3.9974937343358395e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0188, 'grad_norm': 0.35256874561309814, 'learning_rate': 3.9348370927318297e-05, 'epoch': 2.13}\n",
      "{'loss': 0.018, 'grad_norm': 0.16764189302921295, 'learning_rate': 3.87218045112782e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0168, 'grad_norm': 0.10758305341005325, 'learning_rate': 3.809523809523809e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0171, 'grad_norm': 0.20786583423614502, 'learning_rate': 3.7468671679198e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0162, 'grad_norm': 0.07438060641288757, 'learning_rate': 3.6842105263157895e-05, 'epoch': 2.63}\n",
      "{'loss': 0.016, 'grad_norm': 0.26040253043174744, 'learning_rate': 3.6215538847117796e-05, 'epoch': 2.75}\n",
      "{'loss': 0.0157, 'grad_norm': 0.19125886261463165, 'learning_rate': 3.55889724310777e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0148, 'grad_norm': 0.16277797520160675, 'learning_rate': 3.49624060150376e-05, 'epoch': 3.01}\n",
      "{'loss': 0.0138, 'grad_norm': 0.09659475088119507, 'learning_rate': 3.433583959899749e-05, 'epoch': 3.13}\n",
      "{'loss': 0.0137, 'grad_norm': 0.17271360754966736, 'learning_rate': 3.3709273182957394e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0133, 'grad_norm': 0.16908425092697144, 'learning_rate': 3.3082706766917295e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0139, 'grad_norm': 0.08127716183662415, 'learning_rate': 3.24561403508772e-05, 'epoch': 3.51}\n",
      "{'loss': 0.013, 'grad_norm': 0.714630126953125, 'learning_rate': 3.182957393483709e-05, 'epoch': 3.63}\n",
      "{'loss': 0.0127, 'grad_norm': 0.23461157083511353, 'learning_rate': 3.120300751879699e-05, 'epoch': 3.76}\n",
      "{'loss': 0.0124, 'grad_norm': 0.4462873935699463, 'learning_rate': 3.0576441102756894e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0117, 'grad_norm': 0.12951160967350006, 'learning_rate': 2.9949874686716795e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0117, 'grad_norm': 0.29484036564826965, 'learning_rate': 2.9323308270676693e-05, 'epoch': 4.13}\n",
      "{'loss': 0.0111, 'grad_norm': 0.4035625159740448, 'learning_rate': 2.8696741854636594e-05, 'epoch': 4.26}\n",
      "{'loss': 0.011, 'grad_norm': 0.4679381251335144, 'learning_rate': 2.8070175438596492e-05, 'epoch': 4.38}\n",
      "{'loss': 0.011, 'grad_norm': 0.5766641497612, 'learning_rate': 2.7443609022556393e-05, 'epoch': 4.51}\n",
      "{'loss': 0.0109, 'grad_norm': 0.08566691726446152, 'learning_rate': 2.681704260651629e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0107, 'grad_norm': 0.13304999470710754, 'learning_rate': 2.6190476190476192e-05, 'epoch': 4.76}\n",
      "{'loss': 0.01, 'grad_norm': 0.42843276262283325, 'learning_rate': 2.556390977443609e-05, 'epoch': 4.88}\n",
      "{'loss': 0.0104, 'grad_norm': 0.24106353521347046, 'learning_rate': 2.494360902255639e-05, 'epoch': 5.01}\n",
      "{'loss': 0.0099, 'grad_norm': 0.13579237461090088, 'learning_rate': 2.431704260651629e-05, 'epoch': 5.13}\n",
      "{'loss': 0.0098, 'grad_norm': 0.5769079923629761, 'learning_rate': 2.369047619047619e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0095, 'grad_norm': 0.18977093696594238, 'learning_rate': 2.306390977443609e-05, 'epoch': 5.38}\n",
      "{'loss': 0.0094, 'grad_norm': 0.17670069634914398, 'learning_rate': 2.243734335839599e-05, 'epoch': 5.51}\n",
      "{'loss': 0.0093, 'grad_norm': 0.19771161675453186, 'learning_rate': 2.181077694235589e-05, 'epoch': 5.63}\n",
      "{'loss': 0.0095, 'grad_norm': 0.10101767629384995, 'learning_rate': 2.118421052631579e-05, 'epoch': 5.76}\n",
      "{'loss': 0.0094, 'grad_norm': 0.5256723761558533, 'learning_rate': 2.055764411027569e-05, 'epoch': 5.89}\n",
      "{'loss': 0.009, 'grad_norm': 0.08142931014299393, 'learning_rate': 1.993107769423559e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0087, 'grad_norm': 0.40234944224357605, 'learning_rate': 1.9304511278195488e-05, 'epoch': 6.14}\n",
      "{'loss': 0.0084, 'grad_norm': 0.24029821157455444, 'learning_rate': 1.867794486215539e-05, 'epoch': 6.26}\n",
      "{'loss': 0.0087, 'grad_norm': 0.41552871465682983, 'learning_rate': 1.805137844611529e-05, 'epoch': 6.39}\n",
      "{'loss': 0.0085, 'grad_norm': 0.26740485429763794, 'learning_rate': 1.7424812030075188e-05, 'epoch': 6.51}\n",
      "{'loss': 0.0085, 'grad_norm': 0.2446994036436081, 'learning_rate': 1.679824561403509e-05, 'epoch': 6.64}\n",
      "{'loss': 0.0081, 'grad_norm': 0.4010026156902313, 'learning_rate': 1.6171679197994987e-05, 'epoch': 6.76}\n",
      "{'loss': 0.0086, 'grad_norm': 0.6553075909614563, 'learning_rate': 1.554511278195489e-05, 'epoch': 6.89}\n",
      "{'loss': 0.0082, 'grad_norm': 0.09967520087957382, 'learning_rate': 1.4918546365914788e-05, 'epoch': 7.01}\n",
      "{'loss': 0.0078, 'grad_norm': 0.3502156436443329, 'learning_rate': 1.4291979949874687e-05, 'epoch': 7.14}\n",
      "{'loss': 0.0077, 'grad_norm': 0.21311146020889282, 'learning_rate': 1.3665413533834587e-05, 'epoch': 7.26}\n",
      "{'loss': 0.0079, 'grad_norm': 0.12175913900136948, 'learning_rate': 1.3038847117794487e-05, 'epoch': 7.39}\n",
      "{'loss': 0.0071, 'grad_norm': 0.21637873351573944, 'learning_rate': 1.2412280701754386e-05, 'epoch': 7.51}\n",
      "{'loss': 0.0075, 'grad_norm': 0.13190966844558716, 'learning_rate': 1.1785714285714286e-05, 'epoch': 7.64}\n",
      "{'loss': 0.0076, 'grad_norm': 0.1242917999625206, 'learning_rate': 1.1159147869674185e-05, 'epoch': 7.76}\n",
      "{'loss': 0.0078, 'grad_norm': 0.155074343085289, 'learning_rate': 1.0532581453634085e-05, 'epoch': 7.89}\n",
      "{'loss': 0.0074, 'grad_norm': 0.340690940618515, 'learning_rate': 9.906015037593986e-06, 'epoch': 8.01}\n",
      "{'loss': 0.0076, 'grad_norm': 0.12637794017791748, 'learning_rate': 9.279448621553886e-06, 'epoch': 8.14}\n",
      "{'loss': 0.0075, 'grad_norm': 0.23472483456134796, 'learning_rate': 8.652882205513785e-06, 'epoch': 8.26}\n",
      "{'loss': 0.007, 'grad_norm': 0.2792651951313019, 'learning_rate': 8.026315789473685e-06, 'epoch': 8.39}\n",
      "{'loss': 0.0072, 'grad_norm': 0.32071802020072937, 'learning_rate': 7.399749373433584e-06, 'epoch': 8.51}\n",
      "{'loss': 0.0073, 'grad_norm': 0.16700325906276703, 'learning_rate': 6.773182957393484e-06, 'epoch': 8.64}\n",
      "{'loss': 0.007, 'grad_norm': 0.19882084429264069, 'learning_rate': 6.146616541353384e-06, 'epoch': 8.77}\n",
      "{'loss': 0.0072, 'grad_norm': 0.18014821410179138, 'learning_rate': 5.520050125313284e-06, 'epoch': 8.89}\n",
      "{'loss': 0.0069, 'grad_norm': 0.3668885827064514, 'learning_rate': 4.893483709273183e-06, 'epoch': 9.02}\n",
      "{'loss': 0.007, 'grad_norm': 0.15500198304653168, 'learning_rate': 4.266917293233083e-06, 'epoch': 9.14}\n",
      "{'loss': 0.007, 'grad_norm': 0.15483838319778442, 'learning_rate': 3.640350877192983e-06, 'epoch': 9.27}\n",
      "{'loss': 0.0069, 'grad_norm': 0.08955083042383194, 'learning_rate': 3.0137844611528824e-06, 'epoch': 9.39}\n",
      "{'loss': 0.0072, 'grad_norm': 0.2568046748638153, 'learning_rate': 2.387218045112782e-06, 'epoch': 9.52}\n",
      "{'loss': 0.0068, 'grad_norm': 0.17091909050941467, 'learning_rate': 1.7606516290726817e-06, 'epoch': 9.64}\n",
      "{'loss': 0.0066, 'grad_norm': 0.14863179624080658, 'learning_rate': 1.1340852130325815e-06, 'epoch': 9.77}\n",
      "{'loss': 0.0067, 'grad_norm': 0.1020309180021286, 'learning_rate': 5.075187969924812e-07, 'epoch': 9.89}\n",
      "{'train_runtime': 21355.5069, 'train_samples_per_second': 11.966, 'train_steps_per_second': 0.374, 'train_loss': 0.01668532095606763, 'epoch': 9.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7980, training_loss=0.01668532095606763, metrics={'train_runtime': 21355.5069, 'train_samples_per_second': 11.966, 'train_steps_per_second': 0.374, 'train_loss': 0.01668532095606763, 'epoch': 9.99})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O O O O O O O O O O O O']\n",
      "Text: the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\n",
      "Prediction: O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O O O O O O O O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_data(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example geological text\n",
    "geo_texts = [\n",
    "    \"the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\",\n",
    "]\n",
    "\n",
    "# Predict geological entities\n",
    "predictions = predict_entities(geo_texts, model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(predictions)\n",
    "for text, prediction in zip(geo_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model\\\\tokenizer_config.json',\n",
       " './saved_model\\\\special_tokens_map.json',\n",
       " './saved_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained('./saved_model')  # Custom directory for saving\n",
    "# tokenizer.save_pretrained('./saved_model')  # Save tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "save_directory = \"./NER/saved_model\"\n",
    "# Load the tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
