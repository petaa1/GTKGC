{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "def preprocess_and_tokenize(file_path):\n",
    "    \"\"\"\n",
    "    Reads the file, preprocesses it, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    input_texts = []\n",
    "    target_labels = []\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Non-empty line\n",
    "            token, label = line.split()\n",
    "            current_tokens.append(token)\n",
    "            current_labels.append(label)\n",
    "        else:  # Empty line signifies end of a sentence\n",
    "            if current_tokens:\n",
    "                input_texts.append(' '.join(current_tokens))\n",
    "                target_labels.append(' '.join(current_labels))\n",
    "                # Reset for next sentence\n",
    "                current_tokens = []\n",
    "                current_labels = []\n",
    "\n",
    "    # Add the last sentence if the data doesn't end with an empty line\n",
    "    if current_tokens:\n",
    "        input_texts.append(' '.join(current_tokens))\n",
    "        target_labels.append(' '.join(current_labels))\n",
    "    \n",
    "    return input_texts, target_labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = 'NER/Training_Data/AutoLabelledSet.txt'  # Replace with your file path\n",
    "input_texts, target_labels = preprocess_and_tokenize(file_path)\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['around 1070 ma , the region experienced significant volcanic activity .', 'the formation of the supercontinent pannotia occurred approximately 600 ma .', 'during the triassic , around 250 ma , dinosaurs began to dominate .', 'the last glacial maximum occurred roughly 20 ka ago .', 'evidence suggests that the andean orogeny began around 200 ma .']\n",
      "['0 B-GEO_TIME I-GEO_TIME 0 0 0 0 0 0 0 0', '0 0 0 0 0 0 0 0 B-GEO_TIME I-GEO_TIME 0', '0 0 B-TIMESCALE 0 0 B-GEO_TIME I-GEO_TIME 0 0 0 0 0', '0 0 0 0 0 0 B-GEO_TIME I-GEO_TIME 0 0', '0 0 0 0 0 0 0 0 B-GEO_TIME I-GEO_TIME 0']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Initialize lists for sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each entry in the JSON data\n",
    "    for entry in data:\n",
    "        output = entry['output']\n",
    "        label = entry['labels']\n",
    "        \n",
    "        # Join the output list into a string sentence\n",
    "        sentence = ' '.join(output).lower()\n",
    "        \n",
    "        # Join the labels list into a string\n",
    "        label_str = ' '.join(label)\n",
    "        \n",
    "        # Append to lists\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label_str)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_name = \"NER/Training_Data/geotimeLabels.json\"  # Replace with your file path\n",
    "sentences, labels = preprocess_and_tokenize_json(file_name)\n",
    "\n",
    "print(sentences[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25554\n",
      "Number of testing samples: 6389\n",
      "['4.2 local geology the project is underlain entirely by rocks of the coolgardie domain , a sub domain of the kalgoorlie terrane .', 'the abundant crystals or phenocrysts are totally altered feldspars and biotite plus the quartz .', 'recent work by blina resources nl has announced the discovery of numerous diamond bearing palaeochannels on their tenements held in joint venture with the kimberley diamond company immediately to the east of nwds kimberley downs prospects .', 'exploration comprised prospecting , rock sampling and stream sediment geochemistry .', 'historical drilling and surface sampling gold deposit legend historical drilling historical surface sampling cheroona well project 2006 annual report page 26 of 35 5.0 previous work gleneagle gold limited during the previous reporting period from 20th july 2005 to 31st march 2006 , exploration conducted by gleneagle gold limited included the processing of geophysical datasets , investigation and compilation of wamex open file data and the collection of two rock chip samples .']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    input_texts, target_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of testing samples: {len(test_texts)}\")\n",
    "\n",
    "print(train_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and labels.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    targets = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure labels are the same length as inputs\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input IDs: tensor([[    3, 19765,   415,  ...,     0,     0,     0],\n",
      "        [    8, 16346,  6884,  ...,     0,     0,     0],\n",
      "        [ 1100,   161,    57,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   48,  3303,    65,  ...,     0,     0,     0],\n",
      "        [    8, 11508,     9,  ...,     0,     0,     0],\n",
      "        [ 2724,  4012,  2155,  ...,     0,     0,     0]])\n",
      "Training Labels: tensor([[411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 272,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 272,  18,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0]])\n",
      "Testing Input IDs: tensor([[   12,     8,  3457,  ...,     0,     0,     0],\n",
      "        [    3,     9,  4727,  ...,     0,     0,     0],\n",
      "        [ 8282,  1467,    13,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    8,  3731,   159,  ...,     0,     0,     0],\n",
      "        [ 1877,  2122,  2252,  ...,     0,     0,     0],\n",
      "        [18771,    19, 17509,  ...,     0,     0,     0]])\n",
      "Testing Labels: tensor([[411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/t5-base-conll03-english\")\n",
    "\n",
    "# Tokenize training and testing data\n",
    "train_data = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "test_data = tokenize_data(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# Print tokenized data for verification\n",
    "print(\"Training Input IDs:\", train_data['input_ids'])\n",
    "print(\"Training Labels:\", train_data['labels'])\n",
    "print(\"Testing Input IDs:\", test_data['input_ids'])\n",
    "print(\"Testing Labels:\", test_data['labels'])\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NERDataset(train_data, train_data['labels'])\n",
    "test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=12, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"dbmdz/t5-base-conll03-english\").to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=10000,\n",
    "    eval_steps=10000,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Define metrics computation\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Flatten the lists\n",
    "    labels = labels.flatten()\n",
    "    predictions = predictions.flatten()\n",
    "    return {\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization.']\n",
      "Text: the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\n",
      "Prediction: the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example geological text\n",
    "geo_texts = [\n",
    "    \"the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\",\n",
    "]\n",
    "\n",
    "# Predict geological entities\n",
    "predictions = predict_entities(geo_texts, model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(predictions)\n",
    "for text, prediction in zip(geo_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./saved_model')  # Custom directory for saving\n",
    "# tokenizer.save_pretrained('./saved_model')  # Save tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "save_directory = \"./NER/saved_model\"\n",
    "# Load the tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "test_file_path = 'NER/Training_data/EvaluationSet.txt' \n",
    "test_inputs, test_targets = preprocess_and_tokenize(test_file_path)\n",
    "\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee256e54c0848f0b1d6917d4c7edea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0638, 'grad_norm': 0.03641903027892113, 'learning_rate': 2.5e-05, 'epoch': 34.78}\n",
      "{'loss': 0.0078, 'grad_norm': 0.03193062171339989, 'learning_rate': 0.0, 'epoch': 69.57}\n",
      "{'train_runtime': 393.1672, 'train_samples_per_second': 22.891, 'train_steps_per_second': 0.509, 'train_loss': 0.03581991046667099, 'epoch': 69.57}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def retrain(model, tokenizer):\n",
    "    # Preprocess and tokenize the data\n",
    "    file_name = \"NER/Training_Data/geotimeLabels.json\"  # Replace with your file path\n",
    "    input_texts, target_labels = preprocess_and_tokenize_json(file_name)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        input_texts, target_labels, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Tokenize training and testing data\n",
    "    train_data = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "    test_data = tokenize_data(test_texts, test_labels, tokenizer)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = NERDataset(train_data, train_data['labels'])\n",
    "    test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "        # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=100,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        gradient_accumulation_steps=8,\n",
    "        save_steps=10000,\n",
    "        eval_steps=10000,\n",
    "        fp16=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# save_directory = \"./NER/saved_model\"\n",
    "# # Load the tokenizer and model from the saved directory\n",
    "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "# model, tokenizer = retrain(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the Mangaroon Orogeny had deformed and metamorphosed the Gascoyne Complex around 1070 ma, the Bangemall Basin comprising the Edmund and Collier sub-basins, developed between the Pilbara and Yilgarn Cratons.\n",
      "['O O B-STRAT I-STRAT O O O B-STRAT I-STRAT O B-STRAT I-STRAT O O B-LOCATION I-LOCATION O O B-STRAT I-STRAT I-STRAT O O O B-LOCATION O B-LOCATION I-LOCATION']\n"
     ]
    }
   ],
   "source": [
    "texts = \"After the Mangaroon Orogeny had deformed and metamorphosed the Gascoyne Complex around 1070 ma, the Bangemall Basin comprising the Edmund and Collier sub-basins, developed between the Pilbara and Yilgarn Cratons.\"\n",
    "\n",
    "predictions = predict_entities(texts, model, tokenizer)\n",
    "\n",
    "print(texts)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the parsed test data\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[82], line 26\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(texts, true_labels, model, tokenizer)\u001b[0m\n\u001b[0;32m     24\u001b[0m predicted_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m---> 26\u001b[0m     predicted_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredict_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     28\u001b[0m true_labels_flat \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m pred_labels_flat \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[82], line 18\u001b[0m, in \u001b[0;36mpredict_entities\u001b[1;34m(texts, model, tokenizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# Move inputs to GPU\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Generate output sequences\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1510\u001b[0m         input_ids,\n\u001b[0;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1524\u001b[0m     )\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2406\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2403\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2404\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(cur_len, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 2406\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m   2407\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   2408\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2410\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# # Load the tokenizer and model from the saved directory\n",
    "# save_directory = \"./NER/saved_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)  # Generate output sequences\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(texts, true_labels, model, tokenizer):\n",
    "    # predicted_texts = predict_entities(texts, model, tokenizer)\n",
    "    predicted_texts = []\n",
    "    for text in texts:\n",
    "        predicted_texts.append(predict_entities(text, model, tokenizer)[0])\n",
    "    \n",
    "    true_labels_flat = []\n",
    "    pred_labels_flat = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        pred_tokens = predicted_texts[i].split()\n",
    "        true_tokens = true_labels[i].split()\n",
    "        \n",
    "        for j in range(len(true_tokens)):\n",
    "            if true_tokens[j] != 'O':  # Ignore 'O' tokens if necessary\n",
    "                true_labels_flat.append(true_tokens[j])\n",
    "                pred_labels_flat.append(pred_tokens[j] if j < len(pred_tokens) else 'O')\n",
    "\n",
    "    # Convert label strings to integers based on model's label mapping\n",
    "    # label_ids = list(model.config.label2id.values())\n",
    "    # print(label_ids)\n",
    "    # true_labels_flat = [model.config.label2id[label] for label in true_labels_flat]\n",
    "    # pred_labels_flat = [model.config.label2id[label] for label in pred_labels_flat]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "    f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "    f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "    \n",
    "    report = classification_report(\n",
    "        true_labels_flat, pred_labels_flat\n",
    "    )\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the model on the parsed test data\n",
    "evaluate_model(test_inputs[:50], test_targets[:50], model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
