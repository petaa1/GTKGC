{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "def preprocess_and_tokenize(file_path):\n",
    "    \"\"\"\n",
    "    Reads the file, preprocesses it, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    input_texts = []\n",
    "    target_labels = []\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Non-empty line\n",
    "            token, label = line.split()\n",
    "            current_tokens.append(token)\n",
    "            current_labels.append(label)\n",
    "        else:  # Empty line signifies end of a sentence\n",
    "            if current_tokens:\n",
    "                input_texts.append(' '.join(current_tokens))\n",
    "                target_labels.append(' '.join(current_labels))\n",
    "                # Reset for next sentence\n",
    "                current_tokens = []\n",
    "                current_labels = []\n",
    "\n",
    "    # Add the last sentence if the data doesn't end with an empty line\n",
    "    if current_tokens:\n",
    "        input_texts.append(' '.join(current_tokens))\n",
    "        target_labels.append(' '.join(current_labels))\n",
    "    \n",
    "    return input_texts, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n",
      "31943 31943\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = 'NER/Training_Data/AutoLabelledSet.txt'  # Replace with your file path\n",
    "input_texts, target_labels = preprocess_and_tokenize(file_path)\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])\n",
    "print(len(input_texts), len(target_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The geology of the hole was dominated by felsic schists and granites .', 'The mineralisation was characterised by traces of disseminated pyrite with zones of trace pyrrhotite and chalcopyrite in felsic schist .', 'The best mineralisation was intersected in felsic schists below the interpreted position of the VTEM plate model .', 'The geology of the hole was ultramafic schists overlying amphibolite and amphibolitic schists with two 1 m wide weakly sulphidic quartz veins .', 'These quartz veins were characterised by green colouration with traces of magnetite and disseminated pyrite , however contained no anomalous chemistry .']\n",
      "['O O O O O O O O B-ROCK I-ROCK O B-ROCK O', 'O O O O O O O O B-MINERAL O O O O B-MINERAL O B-MINERAL O B-ROCK I-ROCK O', 'O O O O O O B-ROCK I-ROCK O O O O O O O O O O', 'O O O O O O B-ROCK I-ROCK O B-ROCK O B-ROCK I-ROCK O O O O O O O B-ROCK I-ROCK O', 'O B-ROCK I-ROCK O O O O O O O O B-MINERAL O O B-MINERAL O O O O O O O']\n",
      "2001 2001\n"
     ]
    }
   ],
   "source": [
    "test_file = \"NER/Training_Data/EvaluationSet.txt\"\n",
    "test_inputs, test_labels = preprocess_and_tokenize(test_file)\n",
    "print(test_inputs[:5])\n",
    "print(test_labels[:5])\n",
    "print(len(test_inputs), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Initialize lists for sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each entry in the JSON data\n",
    "    for entry in data:\n",
    "        output = entry['output']\n",
    "        label = entry['labels']\n",
    "        \n",
    "        # Join the output list into a string sentence\n",
    "        sentence = ' '.join(output).lower()\n",
    "        \n",
    "        # Join the labels list into a string\n",
    "        label_str = ' '.join(label)\n",
    "        \n",
    "        # Append to lists\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label_str)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['around 1070 ma , the region experienced significant volcanic activity .', 'the formation of the supercontinent pannotia occurred approximately 600 ma .', 'the last glacial maximum occurred roughly 20 ka ago .', 'evidence suggests that the andean orogeny began around 200 ma .', 'the cambrian explosion took place approximately 541 ma .']\n",
      "['O B-GEO_TIME I-GEO_TIME O O O O O O O O', 'O O O O O O O O B-GEO_TIME I-GEO_TIME O', 'O O O O O O B-GEO_TIME I-GEO_TIME O O', 'O O O O O O O O B-GEO_TIME I-GEO_TIME O', 'O B-TIMESCALE O O O O B-GEO_TIME I-GEO_TIME O']\n",
      "87 87\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_name = \"NER/Training_Data/geotimeLabels.json\"  # Replace with your file path\n",
    "geotime_text, geotime_labels = preprocess_and_tokenize_json(file_name)\n",
    "\n",
    "print(geotime_text[:5])\n",
    "print(geotime_labels[:5])\n",
    "print(len(geotime_text), len(geotime_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"NER/Training_Data/geotimeLabels.json\"  # Replace with your file path\n",
    "# with open(file_name, 'r') as json_file:\n",
    "#         data = json.load(json_file)\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     data[i]['labels'] = ['O' if x == '0' else x for x in data[i]['labels']]\n",
    "\n",
    "# with open(file_name, 'w') as json_file:\n",
    "#     json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aalenian', 'abereiddian', 'acadian', 'actonian', 'adelaidean']\n",
      "['B-TIMESCALE', 'B-TIMESCALE', 'B-TIMESCALE', 'B-TIMESCALE', 'B-TIMESCALE']\n",
      "16920 16920\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_name = \"NER/Training_Data/DomainDictionary.json\"  # Replace with your file path\n",
    "dictionary_words, dictionary_labels = preprocess_and_tokenize_json(file_name)\n",
    "\n",
    "print(dictionary_words[:5])\n",
    "print(dictionary_labels[:5])\n",
    "print(len(dictionary_words), len(dictionary_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_words = []\n",
    "dd_labels = []\n",
    "\n",
    "sentence = \"\"\n",
    "labels = \"\"\n",
    "for i in range(len(dictionary_words)):\n",
    "    word = dictionary_words[i]\n",
    "    label = dictionary_labels[i]\n",
    "    if len(sentence.split(\" \")) + len(word.split(\" \")) > 60:\n",
    "        dd_words.append(sentence)\n",
    "        dd_labels.append(labels)\n",
    "        sentence = \"\"\n",
    "        labels = \"\"\n",
    "        sentence += word\n",
    "        labels += label\n",
    "    else:\n",
    "        if sentence == \"\":\n",
    "            sentence += word\n",
    "            labels += label\n",
    "        else:\n",
    "            sentence += \" , \" + word\n",
    "            labels += \" O \" + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aalenian , abereiddian , acadian , actonian , adelaidean , aegean , aeronian , aksayan , aktastinian , alaunian , albertan , albian , aldingian , alexandrian , alportian , altonian , amgan , animikean , anisian , aphebian , aptian , aquitanian , aratauran , archean , archeozoic , arenig , arenigian , arikareean , aritan , arnsbergian , arowhanan', 'artinskian , arundian , asbian , ashgill , asselian , astian , atdabanian , atokan/derryan , atokan , aurelucian , austinian , auversian , awamoan , ayusokkanian , azoic , baigendzinian , bairnsdalian , baishaean , bajocian , bala , balan , balcombian , bananian , baotan , barremian , barstovian , bartonian , bashkirian , basin , batesfordian , bathonian', 'batyrbayan , begudian , bendigonian , berriasian , bithynian , black , blackriveran , blackriverian , blancan , bolindian , bolsovian , boomerangian , bortonian , botomian , braxtonian , bridgerian , brigantian , bulitian , buntsandstein , burdigalian , burrellian , burzyan , caerfai , calabrian , callovian , calymmian , cambrian , campanian , canadaway , canadian , cantabrian', 'capitanian , caradoc , caradocian , carboniferous , carnian , carpentarian , cassinian , castile , castlecliffian , castlemanian , cautleyan , cayugan , cazenovian , cenomanian , cenozoic , chadian , chadronian , chamovnicheskian , changhsingian , changlangpuan , changshanian , changshingian , changxingian , charmouthian , chatauquan , chattian , chazyan , cheltenhamian , cheneyan , cheremshanskian , chesterian', 'chewtonian , chickasawhayan , chokerian , chokierian , chouteau , cincinnatian , cisuralian , clairbornean , clarendonian , clarkforkian , cliffdenian , coahulian , comanchean , conewangan , coniacian , conneautan , costonian , courceyan , cressagian , cretaceous , croixian , cryogenian , cryptic , cryptozoic , dacian , dalanian , danian , darriwilian , datangian , datsonian , dawanian']\n",
      "['B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE', 'B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE', 'B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE', 'B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE', 'B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE O B-TIMESCALE']\n",
      "785 785\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(dd_words[:5])\n",
    "print(dd_labels[:5])\n",
    "print(len(dd_words), len(dd_labels))\n",
    "\n",
    "print(len(dd_words[0].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "#     input_texts, target_labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# print(f\"Number of training samples: {len(train_texts)}\")\n",
    "# print(f\"Number of testing samples: {len(test_texts)}\")\n",
    "\n",
    "# print(train_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and labels.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    targets = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure labels are the same length as inputs\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yuinmery volcanics member , yule granitic complex , yulleroo formation , yulyupunyu granitic gneiss , yumba formation , yumurrpa granophyre , yundi sandy loam , yungkulungu formation , yunkanjini granite , yunta well leucogranite , yununba granite , yurabi formation , yuruga formation , zamia creek siltstone , zamu dolerite , zebra hill suite , zeepaard formation , ziggy monzogranite\n",
      "32815\n",
      "32815\n"
     ]
    }
   ],
   "source": [
    "train_input = [words for words in input_texts] \n",
    "train_input.extend([words for words in geotime_text])\n",
    "train_input.extend([words for words in dd_words])\n",
    "\n",
    "train_labels = [labels for labels in target_labels]\n",
    "train_labels.extend([labels for labels in geotime_labels])\n",
    "train_labels.extend([labels for labels in dd_labels])\n",
    "\n",
    "print(train_input[-1])\n",
    "print(len(train_input))\n",
    "print(len(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input IDs: tensor([    8,     3,  2047,  3727,    15,    15,  3303,  2579,     3,     9,\n",
      "         1561,  5932,    13,  6173,    51,     9,  4638,   954,  4638, 12288,\n",
      "           11,  1413,   109,     9,   162,    26, 23474,  1208, 12288,     3,\n",
      "            6,   147,   521,    77,    57,  4251,  3113,  1225,   975, 24422,\n",
      "          342,     3,     5,     1,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "Training Labels: tensor([  411,   272,    18,  5017,   254,  8015,   411,   411,   411,   411,\n",
      "          411,   411,   272,    18, 26893,   439,    27,    18, 26893,   439,\n",
      "           27,    18, 26893,   439,   411,   411,   272,    18, 26893,   439,\n",
      "           27,    18, 26893,   439,   411,   411,   411,   272,    18, 26893,\n",
      "          439,    27,    18, 26893,   439,   411,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "Testing Input IDs: tensor([[   37,   873,  1863,  ...,     0,     0,     0],\n",
      "        [   37,  7590,  2121,  ...,     0,     0,     0],\n",
      "        [   37,   200,  7590,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   37,  7590,  2121,  ...,     0,     0,     0],\n",
      "        [ 8144,     3,  2568,  ...,     0,     0,     0],\n",
      "        [    3, 22355,  4802,  ...,     0,     0,     0]])\n",
      "Testing Labels: tensor([[411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0],\n",
      "        [411, 411, 411,  ...,   0,   0,   0]])\n",
      "Input Shape: torch.Size([32815, 256])\n",
      "Label Shape: torch.Size([32815, 256])\n",
      "Input Shape: torch.Size([2001, 256])\n",
      "Label Shape: torch.Size([2001, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/t5-base-conll03-english\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "\n",
    "# Tokenize training and testing data\n",
    "train_data = tokenize_data(train_input, train_labels, tokenizer)\n",
    "test_data = tokenize_data(test_inputs, test_labels, tokenizer)\n",
    "\n",
    "\n",
    "# Print tokenized data for verification\n",
    "print(\"Training Input IDs:\", train_data['input_ids'][0])\n",
    "print(\"Training Labels:\", train_data['labels'][0])\n",
    "print(\"Testing Input IDs:\", test_data['input_ids'])\n",
    "print(\"Testing Labels:\", test_data['labels'])\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NERDataset(train_data, train_data['labels'])\n",
    "test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=12, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=12)\n",
    "\n",
    "# Print or return the shape of the inputs and labels\n",
    "input_shape = train_data['input_ids'].shape\n",
    "label_shape = train_data['input_ids'].shape\n",
    "\n",
    "print(f\"Input Shape: {input_shape}\")\n",
    "print(f\"Label Shape: {label_shape}\")\n",
    "\n",
    "# Print or return the shape of the inputs and labels\n",
    "test_input_shape = test_data['input_ids'].shape\n",
    "test_label_shape = test_data['input_ids'].shape\n",
    "\n",
    "print(f\"Input Shape: {test_input_shape}\")\n",
    "print(f\"Label Shape: {test_label_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"dbmdz/t5-base-conll03-english\").to(device)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\").to(device)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=15000,\n",
    "    eval_steps=15000,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Define metrics computation\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Flatten the lists\n",
    "    labels = labels.flatten()\n",
    "    predictions = predictions.flatten()\n",
    "    return {\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1cfeac89f74293b1ecbe0f0b7b885e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1756, 'grad_norm': 0.06519773602485657, 'learning_rate': 4.951219512195122e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0697, 'grad_norm': 0.08146729320287704, 'learning_rate': 4.902439024390244e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0612, 'grad_norm': 0.14215214550495148, 'learning_rate': 4.853658536585366e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0545, 'grad_norm': 0.08234860748052597, 'learning_rate': 4.804878048780488e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0468, 'grad_norm': 0.27755144238471985, 'learning_rate': 4.75609756097561e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0418, 'grad_norm': 0.16214203834533691, 'learning_rate': 4.707317073170732e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0383, 'grad_norm': 0.09076617658138275, 'learning_rate': 4.658536585365854e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0346, 'grad_norm': 0.1473614126443863, 'learning_rate': 4.609756097560976e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0313, 'grad_norm': 0.13732966780662537, 'learning_rate': 4.560975609756098e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0295, 'grad_norm': 0.4108411967754364, 'learning_rate': 4.51219512195122e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0278, 'grad_norm': 0.8251819014549255, 'learning_rate': 4.4634146341463416e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0263, 'grad_norm': 0.19353517889976501, 'learning_rate': 4.414634146341464e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0247, 'grad_norm': 0.25308316946029663, 'learning_rate': 4.3658536585365856e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0237, 'grad_norm': 0.15916532278060913, 'learning_rate': 4.317073170731707e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0221, 'grad_norm': 0.24979911744594574, 'learning_rate': 4.26829268292683e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0214, 'grad_norm': 0.09409663826227188, 'learning_rate': 4.2195121951219514e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0207, 'grad_norm': 0.16785454750061035, 'learning_rate': 4.170731707317073e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0199, 'grad_norm': 0.1826125830411911, 'learning_rate': 4.1219512195121954e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0191, 'grad_norm': 0.5038855075836182, 'learning_rate': 4.073170731707317e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0184, 'grad_norm': 0.5385410785675049, 'learning_rate': 4.0243902439024395e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0181, 'grad_norm': 0.10596621781587601, 'learning_rate': 3.975609756097561e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0168, 'grad_norm': 0.11470825970172882, 'learning_rate': 3.9268292682926835e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0163, 'grad_norm': 0.2866862714290619, 'learning_rate': 3.878048780487805e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0163, 'grad_norm': 0.31896671652793884, 'learning_rate': 3.829268292682927e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0155, 'grad_norm': 0.20058172941207886, 'learning_rate': 3.780487804878049e-05, 'epoch': 2.44}\n",
      "{'loss': 0.016, 'grad_norm': 0.4744608402252197, 'learning_rate': 3.731707317073171e-05, 'epoch': 2.54}\n",
      "{'loss': 0.015, 'grad_norm': 0.25820910930633545, 'learning_rate': 3.682926829268293e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0147, 'grad_norm': 0.695317268371582, 'learning_rate': 3.634146341463415e-05, 'epoch': 2.73}\n",
      "{'loss': 0.0139, 'grad_norm': 0.23962673544883728, 'learning_rate': 3.585365853658537e-05, 'epoch': 2.83}\n",
      "{'loss': 0.0138, 'grad_norm': 0.091487355530262, 'learning_rate': 3.5365853658536584e-05, 'epoch': 2.93}\n",
      "{'loss': 0.0134, 'grad_norm': 0.4089658856391907, 'learning_rate': 3.48780487804878e-05, 'epoch': 3.02}\n",
      "{'loss': 0.0127, 'grad_norm': 0.24123716354370117, 'learning_rate': 3.4390243902439025e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0127, 'grad_norm': 0.08753062039613724, 'learning_rate': 3.390243902439025e-05, 'epoch': 3.22}\n",
      "{'loss': 0.0126, 'grad_norm': 0.16614936292171478, 'learning_rate': 3.3414634146341465e-05, 'epoch': 3.32}\n",
      "{'loss': 0.0125, 'grad_norm': 0.3584211766719818, 'learning_rate': 3.292682926829269e-05, 'epoch': 3.41}\n",
      "{'loss': 0.0119, 'grad_norm': 0.13599534332752228, 'learning_rate': 3.2439024390243906e-05, 'epoch': 3.51}\n",
      "{'loss': 0.012, 'grad_norm': 0.0938546359539032, 'learning_rate': 3.195121951219512e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0111, 'grad_norm': 0.08609065413475037, 'learning_rate': 3.146341463414634e-05, 'epoch': 3.71}\n",
      "{'loss': 0.0112, 'grad_norm': 0.5845369696617126, 'learning_rate': 3.0975609756097564e-05, 'epoch': 3.8}\n",
      "{'loss': 0.0112, 'grad_norm': 0.5005320310592651, 'learning_rate': 3.048780487804878e-05, 'epoch': 3.9}\n",
      "{'loss': 0.0113, 'grad_norm': 0.32572799921035767, 'learning_rate': 3e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0099, 'grad_norm': 0.1260436773300171, 'learning_rate': 2.951219512195122e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0111, 'grad_norm': 0.07814241945743561, 'learning_rate': 2.9024390243902438e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0101, 'grad_norm': 0.11669787764549255, 'learning_rate': 2.8536585365853658e-05, 'epoch': 4.29}\n",
      "{'loss': 0.0102, 'grad_norm': 0.21485039591789246, 'learning_rate': 2.8048780487804882e-05, 'epoch': 4.39}\n",
      "{'loss': 0.01, 'grad_norm': 0.4539237320423126, 'learning_rate': 2.7560975609756102e-05, 'epoch': 4.49}\n",
      "{'loss': 0.0098, 'grad_norm': 0.16414250433444977, 'learning_rate': 2.707317073170732e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0102, 'grad_norm': 0.4448608458042145, 'learning_rate': 2.658536585365854e-05, 'epoch': 4.68}\n",
      "{'loss': 0.01, 'grad_norm': 0.414289265871048, 'learning_rate': 2.609756097560976e-05, 'epoch': 4.78}\n",
      "{'loss': 0.0095, 'grad_norm': 0.13989180326461792, 'learning_rate': 2.5609756097560977e-05, 'epoch': 4.88}\n",
      "{'loss': 0.0094, 'grad_norm': 0.11980913579463959, 'learning_rate': 2.5121951219512197e-05, 'epoch': 4.97}\n",
      "{'loss': 0.0093, 'grad_norm': 0.26655274629592896, 'learning_rate': 2.4634146341463414e-05, 'epoch': 5.07}\n",
      "{'loss': 0.0087, 'grad_norm': 0.38705888390541077, 'learning_rate': 2.4146341463414634e-05, 'epoch': 5.17}\n",
      "{'loss': 0.009, 'grad_norm': 0.11133930832147598, 'learning_rate': 2.3658536585365854e-05, 'epoch': 5.27}\n",
      "{'loss': 0.0088, 'grad_norm': 0.18913382291793823, 'learning_rate': 2.3170731707317075e-05, 'epoch': 5.36}\n",
      "{'loss': 0.0084, 'grad_norm': 0.47419729828834534, 'learning_rate': 2.2682926829268295e-05, 'epoch': 5.46}\n",
      "{'loss': 0.0089, 'grad_norm': 0.23479999601840973, 'learning_rate': 2.2195121951219512e-05, 'epoch': 5.56}\n",
      "{'loss': 0.0086, 'grad_norm': 0.764732301235199, 'learning_rate': 2.1707317073170732e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0082, 'grad_norm': 0.2775663137435913, 'learning_rate': 2.1219512195121953e-05, 'epoch': 5.75}\n",
      "{'loss': 0.0079, 'grad_norm': 0.1114855408668518, 'learning_rate': 2.073170731707317e-05, 'epoch': 5.85}\n",
      "{'loss': 0.0082, 'grad_norm': 0.3082231879234314, 'learning_rate': 2.0243902439024393e-05, 'epoch': 5.95}\n",
      "{'loss': 0.0081, 'grad_norm': 0.11390669643878937, 'learning_rate': 1.975609756097561e-05, 'epoch': 6.05}\n",
      "{'loss': 0.0082, 'grad_norm': 0.4525710344314575, 'learning_rate': 1.926829268292683e-05, 'epoch': 6.14}\n",
      "{'loss': 0.0076, 'grad_norm': 0.08485778421163559, 'learning_rate': 1.878048780487805e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0078, 'grad_norm': 0.6987863779067993, 'learning_rate': 1.8292682926829268e-05, 'epoch': 6.34}\n",
      "{'loss': 0.0076, 'grad_norm': 0.341838538646698, 'learning_rate': 1.7804878048780488e-05, 'epoch': 6.44}\n",
      "{'loss': 0.0079, 'grad_norm': 0.21398736536502838, 'learning_rate': 1.7317073170731708e-05, 'epoch': 6.53}\n",
      "{'loss': 0.0077, 'grad_norm': 0.36979150772094727, 'learning_rate': 1.682926829268293e-05, 'epoch': 6.63}\n",
      "{'loss': 0.0077, 'grad_norm': 0.22013825178146362, 'learning_rate': 1.634146341463415e-05, 'epoch': 6.73}\n",
      "{'loss': 0.0073, 'grad_norm': 0.5178049802780151, 'learning_rate': 1.5853658536585366e-05, 'epoch': 6.83}\n",
      "{'loss': 0.0072, 'grad_norm': 0.16167685389518738, 'learning_rate': 1.5365853658536586e-05, 'epoch': 6.92}\n",
      "{'loss': 0.0072, 'grad_norm': 0.2189718782901764, 'learning_rate': 1.4878048780487805e-05, 'epoch': 7.02}\n",
      "{'loss': 0.0071, 'grad_norm': 0.3129315674304962, 'learning_rate': 1.4390243902439023e-05, 'epoch': 7.12}\n",
      "{'loss': 0.0071, 'grad_norm': 0.07352094352245331, 'learning_rate': 1.3902439024390245e-05, 'epoch': 7.22}\n",
      "{'loss': 0.007, 'grad_norm': 0.18576547503471375, 'learning_rate': 1.3414634146341466e-05, 'epoch': 7.31}\n",
      "{'loss': 0.007, 'grad_norm': 0.18237176537513733, 'learning_rate': 1.2926829268292684e-05, 'epoch': 7.41}\n",
      "{'loss': 0.0069, 'grad_norm': 0.2217712104320526, 'learning_rate': 1.2439024390243903e-05, 'epoch': 7.51}\n",
      "{'loss': 0.0065, 'grad_norm': 0.08434008061885834, 'learning_rate': 1.1951219512195121e-05, 'epoch': 7.61}\n",
      "{'loss': 0.0071, 'grad_norm': 0.23061512410640717, 'learning_rate': 1.1463414634146343e-05, 'epoch': 7.7}\n",
      "{'loss': 0.0071, 'grad_norm': 0.2191106677055359, 'learning_rate': 1.0975609756097562e-05, 'epoch': 7.8}\n",
      "{'loss': 0.0066, 'grad_norm': 0.10458783060312271, 'learning_rate': 1.048780487804878e-05, 'epoch': 7.9}\n",
      "{'loss': 0.0065, 'grad_norm': 0.5344239473342896, 'learning_rate': 1e-05, 'epoch': 8.0}\n",
      "{'loss': 0.0068, 'grad_norm': 0.3577084243297577, 'learning_rate': 9.51219512195122e-06, 'epoch': 8.09}\n",
      "{'loss': 0.0069, 'grad_norm': 0.1657198816537857, 'learning_rate': 9.02439024390244e-06, 'epoch': 8.19}\n",
      "{'loss': 0.0063, 'grad_norm': 0.13818341493606567, 'learning_rate': 8.53658536585366e-06, 'epoch': 8.29}\n",
      "{'loss': 0.0063, 'grad_norm': 0.19938398897647858, 'learning_rate': 8.048780487804879e-06, 'epoch': 8.39}\n",
      "{'loss': 0.0063, 'grad_norm': 0.1626286506652832, 'learning_rate': 7.560975609756098e-06, 'epoch': 8.48}\n",
      "{'loss': 0.0064, 'grad_norm': 0.3230770528316498, 'learning_rate': 7.073170731707317e-06, 'epoch': 8.58}\n",
      "{'loss': 0.0062, 'grad_norm': 0.2209593951702118, 'learning_rate': 6.585365853658537e-06, 'epoch': 8.68}\n",
      "{'loss': 0.0064, 'grad_norm': 0.10792417079210281, 'learning_rate': 6.0975609756097564e-06, 'epoch': 8.78}\n",
      "{'loss': 0.0064, 'grad_norm': 0.1644105613231659, 'learning_rate': 5.609756097560976e-06, 'epoch': 8.87}\n",
      "{'loss': 0.0063, 'grad_norm': 0.11172539740800858, 'learning_rate': 5.121951219512195e-06, 'epoch': 8.97}\n",
      "{'loss': 0.0065, 'grad_norm': 0.5019681453704834, 'learning_rate': 4.634146341463415e-06, 'epoch': 9.07}\n",
      "{'loss': 0.0061, 'grad_norm': 0.11746210604906082, 'learning_rate': 4.146341463414634e-06, 'epoch': 9.17}\n",
      "{'loss': 0.0061, 'grad_norm': 0.19397872686386108, 'learning_rate': 3.6634146341463413e-06, 'epoch': 9.26}\n",
      "{'loss': 0.0063, 'grad_norm': 0.1035725474357605, 'learning_rate': 3.175609756097561e-06, 'epoch': 9.36}\n",
      "{'loss': 0.0062, 'grad_norm': 0.06707506626844406, 'learning_rate': 2.6878048780487807e-06, 'epoch': 9.46}\n",
      "{'loss': 0.0057, 'grad_norm': 0.28830811381340027, 'learning_rate': 2.2e-06, 'epoch': 9.56}\n",
      "{'loss': 0.0061, 'grad_norm': 0.0603216290473938, 'learning_rate': 1.7121951219512198e-06, 'epoch': 9.65}\n",
      "{'loss': 0.0056, 'grad_norm': 0.22536243498325348, 'learning_rate': 1.2243902439024392e-06, 'epoch': 9.75}\n",
      "{'loss': 0.0062, 'grad_norm': 0.2829095423221588, 'learning_rate': 7.365853658536586e-07, 'epoch': 9.85}\n",
      "{'loss': 0.0061, 'grad_norm': 0.31230440735816956, 'learning_rate': 2.487804878048781e-07, 'epoch': 9.95}\n",
      "{'train_runtime': 28490.0689, 'train_samples_per_second': 11.518, 'train_steps_per_second': 0.36, 'train_loss': 0.015258814584918139, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10250, training_loss=0.015258814584918139, metrics={'train_runtime': 28490.0689, 'train_samples_per_second': 11.518, 'train_steps_per_second': 0.36, 'train_loss': 0.015258814584918139, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O O O O O O O O O O O']\n",
      "Text: the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\n",
      "Prediction: O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O O O O O O O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example geological text\n",
    "geo_texts = [\n",
    "    \"the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\",\n",
    "]\n",
    "\n",
    "# Predict geological entities\n",
    "predictions = predict_entities(geo_texts, model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(predictions)\n",
    "for text, prediction in zip(geo_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model\\\\tokenizer_config.json',\n",
       " './saved_model\\\\special_tokens_map.json',\n",
       " './saved_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./saved_model')  # Custom directory for saving\n",
    "tokenizer.save_pretrained('./saved_model')  # Save tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "save_directory = \"./NER/first_saved_model\"\n",
    "# Load the tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-15.1844,  -5.5618,  -1.0842,  ...,   7.9946, -26.2571,  10.1805],\n",
      "         [ 17.0060,  32.5002, -40.2554,  ...,  -9.2514,   6.2497,   5.2842],\n",
      "         [ 11.3847,  -4.8842,   9.0804,  ...,   4.8540,  14.3742,  -5.7740]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# a = model.get_input_embeddings()\n",
    "\n",
    "# # Words you want to get embeddings for\n",
    "# words = [\"geology\"]\n",
    "\n",
    "# # Tokenize the words to get token IDs\n",
    "# inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True).to(device)\n",
    "# print(a(inputs.input_ids))  # (batch_size, sequence_length, hidden_size)\n",
    "# # for i in a:\n",
    "# #     print(i)\n",
    "# # print(model.get_input_embeddings().weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "test_file_path = 'NER/Training_data/EvaluationSet.txt' \n",
    "test_inputs, test_targets = preprocess_and_tokenize(test_file_path)\n",
    "\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "Number of valid entries: 87\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Remove entries with mismatched lengths\n",
    "file_name = \"NER/Training_Data/geotimeLabels.json\"  # Replace with your file path\n",
    "\n",
    "with open(file_name, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "valid_data = []\n",
    "\n",
    "# Iterate through the entries and check the lengths\n",
    "for entry in data:\n",
    "    output = entry['output']\n",
    "    label = entry['labels']\n",
    "    \n",
    "    # Check if lengths are the same\n",
    "    if len(output) == len(label):\n",
    "        valid_data.append(entry)  # Keep only valid entries\n",
    "    else:\n",
    "        print(f\"Removed entry with mismatched lengths: {len(output)} vs {len(label)}\")\n",
    "        print(f\"Output: {output}\")\n",
    "        print(f\"Labels: {label}\")\n",
    "        print()\n",
    "\n",
    "# Print the number of valid entries\n",
    "print(f\"Number of valid entries: {len(valid_data)}\")\n",
    "\n",
    "with open(file_name, 'w') as json_file:\n",
    "    json.dump(valid_data, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def retrain(model, tokenizer):\n",
    "    # Preprocess and tokenize the data\n",
    "    file_name = \"NER/Training_Data/geotimeLabels.json\"  # Replace with your file path\n",
    "    input_texts, target_labels = preprocess_and_tokenize_json(file_name)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        input_texts, target_labels, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Tokenize training and testing data\n",
    "    train_data = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "    test_data = tokenize_data(test_texts, test_labels, tokenizer)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = NERDataset(train_data, train_data['labels'])\n",
    "    test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "        # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=50,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        gradient_accumulation_steps=8,\n",
    "        save_steps=10000,\n",
    "        eval_steps=10000,\n",
    "        fp16=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# save_directory = \"./NER/saved_model\"\n",
    "# # Load the tokenizer and model from the saved directory\n",
    "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "# model, tokenizer = retrain(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formation of the Rocky Mountains occurred around 70 Ma.\n",
      "['O O O O B-LOCATION I-LOCATION O O O B-GEO_TIME I-GEO_TIME O']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "texts = \"The formation of the Rocky Mountains occurred around 70 Ma.\"\n",
    "\n",
    "predictions = predict_entities(texts, model, tokenizer)\n",
    "\n",
    "print(texts)\n",
    "print(predictions)\n",
    "\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6018\n",
      "F1-score (Micro): 0.6018\n",
      "F1-score (Macro): 0.3750\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  B-GEOCATION       0.00      0.00      0.00         0\n",
      "   B-LOCATION       0.76      0.66      0.71      1006\n",
      "    B-MINERAL       0.87      0.73      0.79      1313\n",
      "B-ORE_DEPOSIT       0.82      0.54      0.65       382\n",
      "       B-ROCK       0.85      0.59      0.70      1880\n",
      "      B-STRAT       0.86      0.55      0.67       640\n",
      "  B-TIMESCALE       0.94      0.70      0.81       210\n",
      "   I-LOCATION       0.63      0.57      0.60       686\n",
      "    I-MINERAL       0.62      0.16      0.26        91\n",
      "I-ORE_DEPOSIT       0.78      0.61      0.69       301\n",
      "       I-ROCK       0.79      0.46      0.58       756\n",
      "      I-STRAT       0.76      0.60      0.67       780\n",
      "  I-TIMESCALE       0.00      0.00      0.00         3\n",
      "            O       0.00      0.00      0.00         0\n",
      "            [       0.00      0.00      0.00         0\n",
      "            ]       0.00      0.00      0.00         0\n",
      "           of       0.00      0.00      0.00         0\n",
      " organization       0.00      0.00      0.00         0\n",
      "            |       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.60      8048\n",
      "    macro avg       0.46      0.33      0.38      8048\n",
      " weighted avg       0.81      0.60      0.69      8048\n",
      "\n",
      "Accuracy: 0.7081\n",
      "F1-score (Micro): 0.7081\n",
      "F1-score (Macro): 0.5047\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   B-LOCATION       0.79      0.81      0.80      1006\n",
      "    B-MINERAL       0.88      0.83      0.85      1313\n",
      "B-ORE_DEPOSIT       0.87      0.68      0.76       382\n",
      "       B-ROCK       0.86      0.67      0.75      1880\n",
      "      B-STRAT       0.86      0.66      0.74       640\n",
      "  B-TIMESCALE       0.96      0.86      0.91       210\n",
      "   I-LOCATION       0.79      0.70      0.74       686\n",
      "    I-MINERAL       0.64      0.15      0.25        91\n",
      "I-ORE_DEPOSIT       0.94      0.76      0.84       301\n",
      "       I-ROCK       0.91      0.54      0.68       756\n",
      "      I-STRAT       0.84      0.68      0.75       780\n",
      "  I-TIMESCALE       0.00      0.00      0.00         3\n",
      "            O       0.00      0.00      0.00         0\n",
      "            [       0.00      0.00      0.00         0\n",
      "            ]       0.00      0.00      0.00         0\n",
      "     location       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.71      8048\n",
      "    macro avg       0.58      0.46      0.50      8048\n",
      " weighted avg       0.85      0.71      0.77      8048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# # Load the tokenizer and model from the saved directory\n",
    "save_directory = \"./NER/first_saved_model\"\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "old_model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)  # Generate output sequences\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(texts, true_labels, model, tokenizer):\n",
    "    # predicted_texts = predict_entities(texts, model, tokenizer)\n",
    "    predicted_texts = []\n",
    "    for text in texts:\n",
    "        predicted_texts.append(predict_entities(text, model, tokenizer)[0])\n",
    "    \n",
    "    true_labels_flat = []\n",
    "    pred_labels_flat = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        pred_tokens = predicted_texts[i].split()\n",
    "        true_tokens = true_labels[i].split()\n",
    "        \n",
    "        for j in range(len(true_tokens)):\n",
    "            if true_tokens[j] != 'O':  # Ignore 'O' tokens if necessary\n",
    "                true_labels_flat.append(true_tokens[j])\n",
    "                pred_labels_flat.append(pred_tokens[j] if j < len(pred_tokens) else 'O')\n",
    "\n",
    "    # Convert label strings to integers based on model's label mapping\n",
    "    # label_ids = list(model.config.label2id.values())\n",
    "    # print(label_ids)\n",
    "    # true_labels_flat = [model.config.label2id[label] for label in true_labels_flat]\n",
    "    # pred_labels_flat = [model.config.label2id[label] for label in pred_labels_flat]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "    f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "    f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "    \n",
    "    report = classification_report(\n",
    "        true_labels_flat, pred_labels_flat\n",
    "    )\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the model on the parsed test data\n",
    "evaluate_model(test_inputs, test_targets, model, tokenizer)\n",
    "evaluate_model(test_inputs, test_targets, old_model, old_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
