{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from temporal_taggers.evaluation import merge_tokens, insert_tags_in_raw_text\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BertForTokenClassification\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace\")\n",
    "# To keep tokenization consistent - we use spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Reports to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r\"..\\..\\..\\wamex\\data\"\n",
    "data_path = Path(directory_path)\n",
    "random.seed(42)\n",
    "\n",
    "filepath = data_path / \"wamex_xml\"\n",
    "reports = {}\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(filepath / file, 'r') as f:\n",
    "                reports[file] = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Load the Raw WAMEX XML data - list of sentences\n",
    "wamex_xml_path = data_path / \"wamex_xml_snapshot.json\"\n",
    "with open(wamex_xml_path, 'r') as file:\n",
    "    xml_data = json.load(file)\n",
    "\n",
    "report_list = list(xml_data.keys())\n",
    "while len(reports) < 100:\n",
    "    report_chosen = random.choice(report_list)\n",
    "    sentences = xml_data[report_chosen]\n",
    "    # Check if the report is not already in the reports dictionary\n",
    "    # Select reports with less than 1000 sentences\n",
    "    if sentences and report_chosen not in reports and len(sentences) < 1000:\n",
    "        reports[report_chosen] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reports:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reports: \", len(reports))\n",
    "# for report in reports:\n",
    "#     print(reports[report])\n",
    "#     print(len(reports[report]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprocess Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \" \".join([token.text for token in nlp(text)])\n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    if text[-3:] == \"Mt.\":\n",
    "        text = text[:-3] + \"Mt .\"\n",
    "    else:\n",
    "        text = text.replace(\"Mt.\", \"Mt\")\n",
    "    return text\n",
    "\n",
    "tagged_reports = {}\n",
    "\n",
    "for report in reports:\n",
    "    tagged_reports[report] = {}\n",
    "    for sentence in reports[report]:\n",
    "        tagged_reports[report][sentence] = {\"preprocess\": preprocess_text(clean(sentence))}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found date: 14/12/2001\n"
     ]
    }
   ],
   "source": [
    "def format_date_string(text):\n",
    "    # Regular expression to match:\n",
    "    # 1. Month Day , Year (with extra space before the comma)\n",
    "    # 2. Month Day,Year (without space after the comma)\n",
    "    pattern = re.compile(r\"(\\b\\w+\\s\\d{1,2})\\s?,\\s?(\\d{4})\")\n",
    "    # Replace the pattern with \"Month Day, Year\" with the correct spacing\n",
    "    formatted_text = pattern.sub(r\"\\1, \\2\", text)\n",
    "    return formatted_text\n",
    "\n",
    "def find_date_pattern(text):\n",
    "    # Regular expression for matching the pattern dd/mm/yyyy\n",
    "    date_pattern = re.compile(r\"\\b(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(?:[0-9]{2}|[0-9]{4})\\b\")\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = date_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "def ordinal(n):\n",
    "    return \"%d%s\" % (n, \"th\" if 11 <= n <= 13 else {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\"))\n",
    "\n",
    "def parse_date(found_date):\n",
    "    # Determine if the year is two or four digits\n",
    "    if len(found_date.split('/')[-1]) == 2:\n",
    "        # Assume that two-digit years belong to the 2000s\n",
    "        date_obj = datetime.strptime(found_date, \"%d/%m/%y\")\n",
    "    else:\n",
    "        date_obj = datetime.strptime(found_date, \"%d/%m/%Y\")\n",
    "    \n",
    "    return date_obj\n",
    "\n",
    "# ________________________________________________________________________  \n",
    "\n",
    "def find_month_year_pattern(text):\n",
    "    # Regular expression for matching the pattern MM/YYYY (ensures month is two digits)\n",
    "    month_year_pattern = re.compile(r\"\\b(0[1-9]|1[0-2])/\\d{4}\\b\")\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = month_year_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "def format_month_year(month_year):\n",
    "    # Parse the date string\n",
    "    date_obj = datetime.strptime(month_year, \"%m/%Y\")\n",
    "    # Format as \"Month Year\"\n",
    "    return date_obj.strftime(\"%B %Y\")\n",
    "\n",
    "# ________________________________________________________________________  \n",
    "# Example usage\n",
    "text = \"This report was created on 14/12/2001 for the project.\"\n",
    "found_date = find_date_pattern(text)\n",
    "\n",
    "if found_date:\n",
    "    print(f\"Found date: {found_date}\")\n",
    "else:\n",
    "    print(\"No date pattern found.\")\n",
    "\n",
    "# ________________________________________________________________________\n",
    "\n",
    "for report in tagged_reports:\n",
    "    for text in tagged_reports[report]:\n",
    "        curr = tagged_reports[report][text][\"preprocess\"]\n",
    "\n",
    "        found_date = find_date_pattern(curr)\n",
    "        while found_date:\n",
    "            # Format the date as \"14th December 2001\"\n",
    "            date_obj = parse_date(found_date)\n",
    "            formatted_date = f\"{ordinal(date_obj.day)} {date_obj.strftime('%B')} {date_obj.year}\"\n",
    "            tagged_reports[report][text][\"preprocess\"] = curr.replace(found_date, formatted_date)\n",
    "\n",
    "            curr = tagged_reports[report][text][\"preprocess\"]\n",
    "            found_date = find_date_pattern(curr)\n",
    "            \n",
    "        found_month_year = find_month_year_pattern(curr)\n",
    "        while found_month_year:\n",
    "            # Format the month and year as \"December 2001\"\n",
    "            formatted_month_year = format_month_year(found_month_year)\n",
    "            tagged_reports[report][text][\"preprocess\"] = curr.replace(found_month_year, formatted_month_year)\n",
    "\n",
    "            curr = tagged_reports[report][text][\"preprocess\"]\n",
    "            found_month_year = find_month_year_pattern(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved the tagged reports\n",
    "# with open(\"../Results/tagged_reports.json\", 'w') as file:\n",
    "#     json.dump(tagged_reports, file)\n",
    "\n",
    "# Load the tagged reports\n",
    "with open(\"../Results/tagged_reports.json\", 'r') as file:\n",
    "    tagged_reports = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-GeoEntity NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './Models/6-GeoEntityNER'\n",
    "# Load the tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "def tokenize_data(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_data(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def NER_tagging(tagged_reports, model, tokenizer):\n",
    "    for report in tagged_reports:\n",
    "        for sentence in tagged_reports[report]:\n",
    "            text = tagged_reports[report][sentence][\"preprocess\"]\n",
    "            tagged_reports[report][sentence][\"ner\"] = predict_entities(text, model, tokenizer)\n",
    "    return tagged_reports\n",
    "\n",
    "\n",
    "# Extract entities from the reports\n",
    "tagged_reports = NER_tagging(tagged_reports, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in tagged_reports:\n",
    "    for sentence in tagged_reports[report]:\n",
    "        text = tagged_reports[report][sentence][\"preprocess\"].split()\n",
    "        ner_tags = tagged_reports[report][sentence][\"ner\"][0].split()\n",
    "        if len(text) != len(ner_tags):\n",
    "            if len(text) < len(ner_tags):\n",
    "                ner_tags = ner_tags[:len(text)]\n",
    "            else:\n",
    "                ner_tags = ner_tags + ['O'] * (len(text) - len(ner_tags))\n",
    "            tagged_reports[report][sentence][\"ner\"] = [\" \".join(ner_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal NER (Real-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing():\n",
    "    pass\n",
    "\n",
    "pdb.set_trace = do_nothing\n",
    "\n",
    "def clean_timex_tags(text):\n",
    "    # Regular expression to find nested TIMEX3 tags\n",
    "    # Regular expression patterns to match and clean up spaces\n",
    "    patterns = {\n",
    "        r'<\\s+TIMEX3': r'<TIMEX3',             # Clean up leading spaces before <TIMEX3\n",
    "        r'</TIMEX3\\s+>': r'</TIMEX3>',          # Clean up trailing spaces after </TIMEX3\n",
    "        r'(\\w+)=\"([^\"]*?)\\s+\"': r'\\1=\"\\2\"'      # Clean up spaces inside attributes (from previous example)\n",
    "    }\n",
    "\n",
    "    # Apply each pattern replacement\n",
    "    for pattern, replacement in patterns.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    nested_timex_pattern = re.compile(r'<TIMEX3[^>]*>(<TIMEX3[^>]*>[^<]+</TIMEX3>)</TIMEX3>')\n",
    "        \n",
    "    # Replace the nested TIMEX3 tags with a single TIMEX3 tag\n",
    "    while nested_timex_pattern.search(text):\n",
    "        text = nested_timex_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def run_temporal_model(tagged_reports):\n",
    "    time_model = BertForTokenClassification.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\").to(device)\n",
    "    time_tokenizer = AutoTokenizer.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\", use_fast=False)\n",
    "\n",
    "    id2label = {v: k for k, v in time_model.config.label2id.items()}\n",
    "\n",
    "    temporal_tagged_reports = {}\n",
    "    for report in tagged_reports:\n",
    "        annotation_id = 1\n",
    "        temporal_tagged_reports[report] = {}\n",
    "        for sentence in tagged_reports[report]:\n",
    "            temporal_tagged_reports[report][sentence] = {}\n",
    "            try:\n",
    "                text = tagged_reports[report][sentence][\"preprocess\"]\n",
    "                processed_text = time_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    result = time_model(**processed_text)\n",
    "\n",
    "                classification = torch.argmax(result[0], dim=2)\n",
    "\n",
    "                # Merge the tokens\n",
    "                merged_tokens = merge_tokens(processed_text[\"input_ids\"][0], classification[0], id2label, time_tokenizer)\n",
    "                annotated_text, annotation_id = insert_tags_in_raw_text(text, merged_tokens, annotation_id)\n",
    "                annotated_text = clean_timex_tags(annotated_text)\n",
    "                temporal_tagged_reports[report][sentence][\"temporal_tagger\"] = annotated_text\n",
    "                print(annotated_text)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing the text: {sentence}\")\n",
    "                temporal_tagged_reports[report][sentence][\"temporal_tagger\"] = tagged_reports[report][sentence][\"preprocess\"]\n",
    "                continue\n",
    "    return temporal_tagged_reports\n",
    "\n",
    "temporal_tagged = run_temporal_model(tagged_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert TIMEX3 format into BIO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timex_and_spans(text):\n",
    "    # Regular expression to find all TIMEX3 tags and extract their content\n",
    "    timex_pattern = re.compile(r'<TIMEX3[^>]*>([^<]+)</TIMEX3>')\n",
    "    \n",
    "    timex_values = []\n",
    "    spans = []\n",
    "    types = []\n",
    "    cleaned_text = text\n",
    "    match = timex_pattern.search(cleaned_text)\n",
    "    \n",
    "    # Find all TIMEX3 tags in the text\n",
    "    # for match in timex_pattern.finditer(cleaned_text):\n",
    "    while match:\n",
    "        timex_value = match.group(1)\n",
    "        timex_values.append(timex_value)\n",
    "\n",
    "        type_pattern = re.compile(r'type=\"([^\"]*)\"')\n",
    "        type = type_pattern.search(match.group(0)).group(1)\n",
    "        types.append(type)\n",
    "        \n",
    "        # Calculate the start and end span in the original text\n",
    "        start_span = match.span()[0]\n",
    "        end_span = start_span + len(timex_value)\n",
    "        spans.append((start_span, end_span))\n",
    "        \n",
    "        # Update the cleaned text by removing the TIMEX3 tag\n",
    "        cleaned_text = cleaned_text.replace(match.group(0), timex_value)\n",
    "        match = timex_pattern.search(cleaned_text)\n",
    "        \n",
    "    return timex_values, cleaned_text, spans, types\n",
    "\n",
    "\n",
    "# Extract TIMEX3 tags and their spans from the text\n",
    "for report in tagged_reports:\n",
    "    for sentence in tagged_reports[report]:\n",
    "        time_tagged_sentence = temporal_tagged[report][sentence][\"temporal_tagger\"]\n",
    "        contains_real_time = \"TIMEX3\" in time_tagged_sentence\n",
    "\n",
    "        tag_labels = [\"O\"] * len(tagged_reports[report][sentence][\"preprocess\"].split())\n",
    "\n",
    "        if contains_real_time:\n",
    "            timex_values, cleaned_text, spans, types = extract_timex_and_spans(time_tagged_sentence)\n",
    "            doc = nlp(cleaned_text.strip())\n",
    "            numOfTemporals = len(timex_values)\n",
    "            temporalNum = 0\n",
    "            span = spans[temporalNum]\n",
    "\n",
    "            for i, word in enumerate(doc):\n",
    "                idx = word.idx\n",
    "                endIdx = idx + len(word.text)\n",
    "                if idx >= span[0] and endIdx <= span[1]:\n",
    "                    if idx == span[0]:\n",
    "                        tag_labels[i] = \"B-DATE\"\n",
    "                    else:\n",
    "                        tag_labels[i] = \"I-DATE\"\n",
    "                if idx > span[1]:\n",
    "                    temporalNum += 1\n",
    "                    if temporalNum < len(spans):\n",
    "                        span = spans[temporalNum]\n",
    "                        \n",
    "        tagged_reports[report][sentence][\"time_tagged\"] = \" \".join(tag_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geological Time NER - Rule Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-TIMESCALE I-TIMESCALE I-TIMESCALE I-TIMESCALE O\n"
     ]
    }
   ],
   "source": [
    "# Text should be already preprocessed by spacey\n",
    "def geological_timescale_ner(text):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Define the geological timescales pattern\n",
    "    timescale_pattern = r'\\b~?\\d+(?:[.,]?\\d+)?(?:\\s*(?:to|-)\\s*~?\\d+(?:[.,]?\\d+)?)?\\s*(?:Ma|ka|Ga|MYA|KYA)\\b'\n",
    "    \n",
    "    # Extract geological timescales and their positions\n",
    "    geological_timescales = re.finditer(timescale_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Create a dictionary to store geological timescale spans\n",
    "    geo_timescale_spans = set()\n",
    "\n",
    "\n",
    "    for match in geological_timescales:\n",
    "        start, end = match.span()\n",
    "        if start != 0 and text[start-1] == \"~\":\n",
    "            start -= 1\n",
    "        geo_timescale_spans.add((start, end))\n",
    "    \n",
    "    # Prepare to format the output\n",
    "    tokens_with_labels = []\n",
    "\n",
    "    token_start = 0\n",
    "    token_end = -1\n",
    "\n",
    "    # Process tokens and assign labels\n",
    "    for token in tokens:\n",
    "        token_text = token\n",
    "        token_start = token_end + 1\n",
    "        token_end = token_start + len(token_text)\n",
    "        token_label = 'O'\n",
    "\n",
    "        for start, end in geo_timescale_spans:\n",
    "            if token_start >= start and token_end <= end:\n",
    "                if (token_start == start):\n",
    "                    token_label = f'B-TIMESCALE'\n",
    "                else:\n",
    "                    token_label = f'I-TIMESCALE'\n",
    "                break\n",
    "        \n",
    "        tokens_with_labels.append(f\"{token_label}\")\n",
    "    \n",
    "    return \" \".join(tokens_with_labels)\n",
    "\n",
    "# Sample text from geological surveys\n",
    "text = \"Mapping and geochronology by the Geological Society of Australia ( Arriens , 1971 ) reveal that the granitic rocks in the western part of the Yalgoo 1:250,000 map sheet are in the order of 2,800 to 3,000 ma .\"\n",
    "\n",
    "# Format the NER output\n",
    "ner = geological_timescale_ner(text)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in tagged_reports:\n",
    "    for sentence in tagged_reports[report]:\n",
    "        text = tagged_reports[report][sentence][\"preprocess\"]\n",
    "        ner = geological_timescale_ner(text)\n",
    "        tagged_reports[report][sentence][\"geotime_tagged\"] = ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O\n",
      "O O O O O O O O O O O O O\n",
      "['O O O O O O O O O B-LOCATION I-LOCATION I-LOCATION O']\n",
      "Managed By : GME Resources Ltd Level 2 907 Canning Highway Mt .\n"
     ]
    }
   ],
   "source": [
    "for report in tagged_reports:\n",
    "    for sentence in tagged_reports[report]:\n",
    "        print(tagged_reports[report][sentence][\"geotime_tagged\"])\n",
    "        print(tagged_reports[report][sentence][\"time_tagged\"])\n",
    "        print(tagged_reports[report][sentence][\"ner\"])\n",
    "        print(tagged_reports[report][sentence][\"preprocess\"])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the tagged reports\n",
    "# with open(\"../Results/tagged_reports_all.json\", 'w') as file:\n",
    "#     json.dump(tagged_reports, file)\n",
    "\n",
    "# Load the tagged reports\n",
    "with open(\"../Results/tagged_reports_all.json\", 'r') as file:\n",
    "    tagged_reports = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types = [\"B-DATE\", \"I-DATE\", \"B-TIMESCALE\", \"I-TIMESCALE\", \"B-ROCK\", \"I-ROCK\", \"B-LOCATION\", \"I-LOCATION\",\n",
    "                \"B-MINERAL\", \"I-MINERAL\", \"B-STRAT\", \"I-STRAT\", \"B-ORE_DEPOSIT\", \"I-ORE_DEPOSIT\"]\n",
    "\n",
    "tagged_reports_combined = {}\n",
    "\n",
    "for report in tagged_reports:\n",
    "    tagged_reports_combined[report] = {}\n",
    "    for sentence in tagged_reports[report]:\n",
    "        tagged_reports_combined[report][sentence] = {}\n",
    "        combined_tags = [\"O\"] * len(tagged_reports[report][sentence][\"preprocess\"].split())\n",
    "        six_geo_tags = tagged_reports[report][sentence][\"ner\"][0].split()\n",
    "        time_tags = tagged_reports[report][sentence][\"time_tagged\"].split()\n",
    "        geotime_tags = tagged_reports[report][sentence][\"geotime_tagged\"].split()\n",
    "        for i in range(len(combined_tags)):\n",
    "            geo_tag = six_geo_tags[i]\n",
    "            if geo_tag != \"O\" and geo_tag in entity_types:\n",
    "                combined_tags[i] = geo_tag\n",
    "            time_tag = time_tags[i]\n",
    "            if time_tag != \"O\" and time_tag in entity_types:\n",
    "                combined_tags[i] = time_tag\n",
    "            geo_tag = geotime_tags[i]\n",
    "            if geo_tag != \"O\" and geo_tag in entity_types:\n",
    "                combined_tags[i] = geo_tag\n",
    "        tagged_reports_combined[report][sentence][\"combined\"] = \" \".join(combined_tags)\n",
    "        tagged_reports_combined[report][sentence][\"preprocess\"] = tagged_reports[report][sentence][\"preprocess\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the tagged reports\n",
    "# with open(\"../Results/tagged_reports_combined_results.json\", 'w') as file:\n",
    "#     json.dump(tagged_reports_combined, file)\n",
    "\n",
    "# Load the tagged reports\n",
    "with open(\"../Results/tagged_reports_combined_results.json\", 'r') as file:\n",
    "    tagged_reports_combined = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- Bar Graph\n",
    "- Num of entities per entities types + unique num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_entities = {'ROCK': 0, 'LOCATION': 0, 'MINERAL': 0, 'STRAT': 0, 'ORE_DEPOSIT': 0, 'DATE': 0, 'TIMESCALE': 0}\n",
    "\n",
    "for report in tagged_reports_combined:\n",
    "    for sentence in tagged_reports_combined[report]:\n",
    "        text = tagged_reports_combined[report][sentence][\"preprocess\"].split()\n",
    "        ner_tags = tagged_reports_combined[report][sentence][\"combined\"].split()\n",
    "\n",
    "        for i in range(len(text)):\n",
    "            if ner_tags[i] != \"O\":\n",
    "                num_of_entities[ner_tags[i]] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
