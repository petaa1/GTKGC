{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from temporal_taggers.evaluation import merge_tokens, insert_tags_in_raw_text\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BertForTokenClassification\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace\")\n",
    "# To keep tokenization consistent - we use spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Reports to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r\"..\\..\\..\\wamex\\data\"\n",
    "data_path = Path(directory_path)\n",
    "random.seed(42)\n",
    "\n",
    "filepath = data_path / \"wamex_xml\"\n",
    "reports = {}\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(filepath / file, 'r') as f:\n",
    "                reports[file] = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Load the Raw WAMEX XML data - list of sentences\n",
    "wamex_xml_path = data_path / \"wamex_xml_snapshot.json\"\n",
    "with open(wamex_xml_path, 'r') as file:\n",
    "    xml_data = json.load(file)\n",
    "\n",
    "report_list = list(xml_data.keys())\n",
    "while len(reports) < 100:\n",
    "    report_chosen = random.choice(report_list)\n",
    "    sentences = xml_data[report_chosen]\n",
    "    # Check if the report is not already in the reports dictionary\n",
    "    # Select reports with less than 1000 sentences\n",
    "    if sentences and report_chosen not in reports and len(sentences) < 1000:\n",
    "        reports[report_chosen] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reports:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reports: \", len(reports))\n",
    "# for report in reports:\n",
    "#     print(reports[report])\n",
    "#     print(len(reports[report]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprocess Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \" \".join([token.text for token in nlp(text)])\n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    if text[-3:] == \"Mt.\":\n",
    "        text = text[:-3] + \"Mt .\"\n",
    "    else:\n",
    "        text = text.replace(\"Mt.\", \"Mt\")\n",
    "    return text\n",
    "\n",
    "tagged_reports = {}\n",
    "\n",
    "for report in reports:\n",
    "    tagged_reports[report] = {}\n",
    "    for sentence in reports[report]:\n",
    "        tagged_reports[report][sentence] = {\"preprocess\": preprocess_text(clean(sentence))}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found date: 14/12/2001\n"
     ]
    }
   ],
   "source": [
    "def format_date_string(text):\n",
    "    # Regular expression to match:\n",
    "    # 1. Month Day , Year (with extra space before the comma)\n",
    "    # 2. Month Day,Year (without space after the comma)\n",
    "    pattern = re.compile(r\"(\\b\\w+\\s\\d{1,2})\\s?,\\s?(\\d{4})\")\n",
    "    # Replace the pattern with \"Month Day, Year\" with the correct spacing\n",
    "    formatted_text = pattern.sub(r\"\\1, \\2\", text)\n",
    "    return formatted_text\n",
    "\n",
    "def find_date_pattern(text):\n",
    "    # Regular expression for matching the pattern dd/mm/yyyy\n",
    "    date_pattern = re.compile(r\"\\b(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(?:[0-9]{2}|[0-9]{4})\\b\")\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = date_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "def ordinal(n):\n",
    "    return \"%d%s\" % (n, \"th\" if 11 <= n <= 13 else {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\"))\n",
    "\n",
    "def parse_date(found_date):\n",
    "    # Determine if the year is two or four digits\n",
    "    if len(found_date.split('/')[-1]) == 2:\n",
    "        # Assume that two-digit years belong to the 2000s\n",
    "        date_obj = datetime.strptime(found_date, \"%d/%m/%y\")\n",
    "    else:\n",
    "        date_obj = datetime.strptime(found_date, \"%d/%m/%Y\")\n",
    "    \n",
    "    return date_obj\n",
    "\n",
    "# ________________________________________________________________________  \n",
    "\n",
    "def find_month_year_pattern(text):\n",
    "    # Regular expression for matching the pattern MM/YYYY (ensures month is two digits)\n",
    "    month_year_pattern = re.compile(r\"\\b(0[1-9]|1[0-2])/\\d{4}\\b\")\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = month_year_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "def format_month_year(month_year):\n",
    "    # Parse the date string\n",
    "    date_obj = datetime.strptime(month_year, \"%m/%Y\")\n",
    "    # Format as \"Month Year\"\n",
    "    return date_obj.strftime(\"%B %Y\")\n",
    "\n",
    "# ________________________________________________________________________  \n",
    "# Example usage\n",
    "text = \"This report was created on 14/12/2001 for the project.\"\n",
    "found_date = find_date_pattern(text)\n",
    "\n",
    "if found_date:\n",
    "    print(f\"Found date: {found_date}\")\n",
    "else:\n",
    "    print(\"No date pattern found.\")\n",
    "\n",
    "# ________________________________________________________________________\n",
    "\n",
    "for report in tagged_reports:\n",
    "    for text in tagged_reports[report]:\n",
    "        curr = tagged_reports[report][text][\"preprocess\"]\n",
    "\n",
    "        found_date = find_date_pattern(curr)\n",
    "        while found_date:\n",
    "            # Format the date as \"14th December 2001\"\n",
    "            date_obj = parse_date(found_date)\n",
    "            formatted_date = f\"{ordinal(date_obj.day)} {date_obj.strftime('%B')} {date_obj.year}\"\n",
    "            tagged_reports[report][text][\"preprocess\"] = curr.replace(found_date, formatted_date)\n",
    "\n",
    "            curr = tagged_reports[report][text][\"preprocess\"]\n",
    "            found_date = find_date_pattern(curr)\n",
    "            \n",
    "        found_month_year = find_month_year_pattern(curr)\n",
    "        while found_month_year:\n",
    "            # Format the month and year as \"December 2001\"\n",
    "            formatted_month_year = format_month_year(found_month_year)\n",
    "            tagged_reports[report][text][\"preprocess\"] = curr.replace(found_month_year, formatted_month_year)\n",
    "\n",
    "            curr = tagged_reports[report][text][\"preprocess\"]\n",
    "            found_month_year = find_month_year_pattern(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved the tagged reports\n",
    "with open(\"../Results/tagged_reports.json\", 'w') as file:\n",
    "    json.dump(tagged_reports, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-GeoEntity NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './Models/6-GeoEntityNER'\n",
    "# Load the tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "def tokenize_data(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_data(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True).to(device)\n",
    "\n",
    "def extract_reports(tagged_reports, model, tokenizer):\n",
    "    for report in tagged_reports:\n",
    "        for sentence in tagged_reports[report]:\n",
    "            tagged_reports[report][sentence][\"ner\"] = predict_entities(tagged_reports[report][sentence][\"preprocess\"], model, tokenizer)\n",
    "    return tagged_reports\n",
    "\n",
    "# Extract entities from the reports\n",
    "# tagged_reports = extract_reports(tagged_reports, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal NER (Real-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 41\u001b[0m\n\u001b[0;32m     29\u001b[0m     time_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msatyaalmasian/temporal_tagger_BERT_tokenclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# id2label = {v: k for k, v in time_model.config.label2id.items()}\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# temporal_tagged_reports = {}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#         text = tagged_reports[report][sentence][\"preprocess\"]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#         processed_text = time_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mrun_temporal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged_reports\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mrun_temporal_model\u001b[1;34m(tagged_reports)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_temporal_model\u001b[39m(tagged_reports):\n\u001b[0;32m     28\u001b[0m     time_model \u001b[38;5;241m=\u001b[39m BertForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msatyaalmasian/temporal_tagger_BERT_tokenclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 29\u001b[0m     time_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msatyaalmasian/temporal_tagger_BERT_tokenclassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "def do_nothing():\n",
    "    pass\n",
    "\n",
    "pdb.set_trace = do_nothing\n",
    "\n",
    "def clean_timex_tags(text):\n",
    "    # Regular expression to find nested TIMEX3 tags\n",
    "    # Regular expression patterns to match and clean up spaces\n",
    "    patterns = {\n",
    "        r'<\\s+TIMEX3': r'<TIMEX3',             # Clean up leading spaces before <TIMEX3\n",
    "        r'</TIMEX3\\s+>': r'</TIMEX3>',          # Clean up trailing spaces after </TIMEX3\n",
    "        r'(\\w+)=\"([^\"]*?)\\s+\"': r'\\1=\"\\2\"'      # Clean up spaces inside attributes (from previous example)\n",
    "    }\n",
    "\n",
    "    # Apply each pattern replacement\n",
    "    for pattern, replacement in patterns.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    nested_timex_pattern = re.compile(r'<TIMEX3[^>]*>(<TIMEX3[^>]*>[^<]+</TIMEX3>)</TIMEX3>')\n",
    "        \n",
    "    # Replace the nested TIMEX3 tags with a single TIMEX3 tag\n",
    "    while nested_timex_pattern.search(text):\n",
    "        text = nested_timex_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def run_temporal_model(tagged_reports):\n",
    "    time_model = BertForTokenClassification.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\").to(device)\n",
    "    time_tokenizer = AutoTokenizer.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\", use_fast=False)\n",
    "\n",
    "    id2label = {v: k for k, v in time_model.config.label2id.items()}\n",
    "\n",
    "    temporal_tagged_reports = {}\n",
    "    for report in tagged_reports:\n",
    "        annotation_id = 1\n",
    "        temporal_tagged_reports[report] = {}\n",
    "        for sentence in tagged_reports[report]:\n",
    "            try:\n",
    "                text = tagged_reports[report][sentence][\"preprocess\"]\n",
    "                processed_text = time_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    result = time_model(**processed_text)\n",
    "\n",
    "                classification = torch.argmax(result[0], dim=2)\n",
    "\n",
    "                # Merge the tokens\n",
    "                merged_tokens = merge_tokens(processed_text[\"input_ids\"][0], classification[0], id2label, time_tokenizer)\n",
    "                annotated_text, annotation_id = insert_tags_in_raw_text(text, merged_tokens, annotation_id)\n",
    "                annotated_text = clean_timex_tags(annotated_text)\n",
    "                temporal_tagged_reports[report][sentence][\"temporal_tagger\"] = annotated_text\n",
    "                print(annotated_text)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing the text: {sentence}\")\n",
    "                temporal_tagged_reports[report][sentence][\"temporal_tagger\"] = tagged_reports[report][sentence][\"preprocess\"]\n",
    "                continue\n",
    "    return temporal_tagged_reports\n",
    "\n",
    "temporal_tagged = run_temporal_model(tagged_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert TIMEX3 format into BIO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timex_and_spans(text):\n",
    "    # Regular expression to find all TIMEX3 tags and extract their content\n",
    "    timex_pattern = re.compile(r'<TIMEX3[^>]*>([^<]+)</TIMEX3>')\n",
    "    \n",
    "    timex_values = []\n",
    "    spans = []\n",
    "    types = []\n",
    "    cleaned_text = text\n",
    "    match = timex_pattern.search(cleaned_text)\n",
    "    \n",
    "    # Find all TIMEX3 tags in the text\n",
    "    # for match in timex_pattern.finditer(cleaned_text):\n",
    "    while match:\n",
    "        timex_value = match.group(1)\n",
    "        timex_values.append(timex_value)\n",
    "\n",
    "        type_pattern = re.compile(r'type=\"([^\"]*)\"')\n",
    "        type = type_pattern.search(match.group(0)).group(1)\n",
    "        types.append(type)\n",
    "        \n",
    "        # Calculate the start and end span in the original text\n",
    "        start_span = match.span()[0]\n",
    "        end_span = start_span + len(timex_value)\n",
    "        spans.append((start_span, end_span))\n",
    "        \n",
    "        # Update the cleaned text by removing the TIMEX3 tag\n",
    "        cleaned_text = cleaned_text.replace(match.group(0), timex_value)\n",
    "        match = timex_pattern.search(cleaned_text)\n",
    "        \n",
    "    return timex_values, cleaned_text, spans, types\n",
    "\n",
    "\n",
    "# Extract TIMEX3 tags and their spans from the text\n",
    "for report in tagged_reports:\n",
    "    for sentence in tagged_reports[report]:\n",
    "        time_tagged_sentence = temporal_tagged[report][sentence][\"time_tagged\"]\n",
    "        contains_real_time = \"TIMEX3\" in time_tagged_sentence\n",
    "\n",
    "        if contains_real_time:\n",
    "            timex_values, cleaned_text, spans, types = extract_timex_and_spans(time_tagged_sentence)\n",
    "            doc = nlp(cleaned_text.strip())\n",
    "            numOfTemporals = len(timex_values)\n",
    "            temporalNum = 0\n",
    "            span = spans[temporalNum]\n",
    "\n",
    "            tag_labels = [\"O\"] * len(doc)\n",
    "\n",
    "            for i, word in enumerate(doc):\n",
    "                idx = word.idx\n",
    "                endIdx = idx + len(word.text)\n",
    "                if idx >= span[0] and endIdx <= span[1]:\n",
    "                    if idx == span[0]:\n",
    "                        tag_labels[i] = \"B-DATE\"\n",
    "                    else:\n",
    "                        tag_labels[i] = \"I-DATE\"\n",
    "                if idx > span[1]:\n",
    "                    temporalNum += 1\n",
    "                    if temporalNum < len(spans):\n",
    "                        span = spans[temporalNum]\n",
    "                        \n",
    "        tagged_reports[report][sentence][\"time_tagged\"] = tag_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geological Time NER - Rule Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('50', 'CARDINAL'), ('New\\nForest', 'ORG'), ('the 25 September 2006', 'DATE'), ('2000 ka', 'GEOLOGICAL_TIME'), ('the Murchison Region of Western Australia', 'LOC'), ('2000', 'DATE'), ('the Department of Industry and Resources', 'ORG'), ('Midwest', 'LOC'), ('50 Ma', 'GEOLOGICAL_TIME'), ('the 26 September 2005', 'DATE')}\n"
     ]
    }
   ],
   "source": [
    "def spacy_large_ner(document):\n",
    "    # Extract named entities using spaCy\n",
    "    entities = {(ent.text.strip(), ent.label_) for ent in nlp(document).ents}\n",
    "    \n",
    "    # Extract geological timescales\n",
    "    pattern = r'\\b\\d+(?:[\\.,]\\d+)?\\s*(?:Ma|ka|Ga|MYA|KYA)\\b'\n",
    "    matches = re.finditer(pattern, document, re.IGNORECASE)\n",
    "    geological_timescales = {(match.group().strip(), 'GEOLOGICAL_TIME') for match in matches}\n",
    "    \n",
    "    # Combine geological timescales with named entities\n",
    "    all_entities = entities.union(geological_timescales)\n",
    "    \n",
    "    return all_entities\n",
    "\n",
    "# Sample text from geological surveys\n",
    "text = \"\"\"\n",
    "This report has been prepared as an investigation of the Mt Aubrey tenement, as part of Midwest’s New\n",
    "Forest project in the Murchison Region of Western Australia. The report is presented as an Annual Report\n",
    "to be submitted to the Department of Industry and Resources as part of the conditions of the granting of\n",
    "E09/1004 and covers the period from the 26 September 2005 to the 25 September 2006. We also have dates like\n",
    "50 Ma and 2000 ka which are geological timescales.\n",
    "\"\"\"\n",
    "\n",
    "# Extract entities and geological timescales\n",
    "print(spacy_large_ner(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mapping O', 'and O', 'geochronology O', 'by O', '100 B-GEO_TIME', '- I-GEO_TIME', '2000 I-GEO_TIME', 'Ma I-GEO_TIME', 'the O', 'Geological O', 'Society O', 'of O', 'Australia O', '( O', 'Arriens O', ', O', '1971 O', ') O', 'reveal O', 'that O', 'the O', 'granitic O', 'rocks O', 'in O', 'the O', 'western O', 'part O', 'of O', 'the O', 'Yalgoo O', '1:250,000 O', 'map O', 'sheet O', 'are O', 'in O', 'the O', 'order O', 'of O', '2,800 B-GEO_TIME', 'to I-GEO_TIME', '3,000 I-GEO_TIME', 'ma I-GEO_TIME', '. O']\n"
     ]
    }
   ],
   "source": [
    "# Text should be already preprocessed by spacey\n",
    "def format_ner_output(text):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Define the geological timescales pattern\n",
    "    timescale_pattern = r'\\b\\d+(?:[\\.,]\\d+)?(?:\\s*(?:to|-)\\s*\\d+(?:[\\.,]\\d+)?)?\\s*(?:Ma|ka|Ga|MYA|KYA)\\b'\n",
    "    \n",
    "    # Extract geological timescales and their positions\n",
    "    geological_timescales = re.finditer(timescale_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Create a dictionary to store geological timescale spans\n",
    "    geo_timescale_spans = set()\n",
    "\n",
    "    for match in geological_timescales:\n",
    "        start, end = match.span()\n",
    "        geo_timescale_spans.add((start, end))\n",
    "    \n",
    "    # Prepare to format the output\n",
    "    tokens_with_labels = []\n",
    "\n",
    "    token_start = 0\n",
    "    token_end = -1\n",
    "\n",
    "    # Process tokens and assign labels\n",
    "    for token in tokens:\n",
    "        token_text = token\n",
    "        token_start = token_end + 1\n",
    "        token_end = token_start + len(token_text)\n",
    "        token_label = 'O'\n",
    "\n",
    "        for start, end in geo_timescale_spans:\n",
    "            if token_start >= start and token_end <= end:\n",
    "                if (token_start == start):\n",
    "                    token_label = f'B-GEO_TIME'\n",
    "                else:\n",
    "                    token_label = f'I-GEO_TIME'\n",
    "                break\n",
    "        \n",
    "        tokens_with_labels.append(f\"{token_text} {token_label}\")\n",
    "    \n",
    "    return tokens_with_labels\n",
    "\n",
    "# Sample text from geological surveys\n",
    "text = \"Mapping and geochronology by 100 - 2000 Ma the Geological Society of Australia ( Arriens , 1971 ) reveal that the granitic rocks in the western part of the Yalgoo 1:250,000 map sheet are in the order of 2,800 to 3,000 ma .\"\n",
    "\n",
    "# Format the NER output\n",
    "ner = format_ner_output(text)\n",
    "print(ner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
