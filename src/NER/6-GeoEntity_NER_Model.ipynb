{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 6-GeoEntity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(file_path):\n",
    "    \"\"\"\n",
    "    Reads the file, preprocesses it, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    input_texts = []\n",
    "    target_labels = []\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Non-empty line\n",
    "            token, label = line.split()\n",
    "            current_tokens.append(token)\n",
    "            current_labels.append(label)\n",
    "        else:  # Empty line signifies end of a sentence\n",
    "            if current_tokens:\n",
    "                input_texts.append(' '.join(current_tokens))\n",
    "                target_labels.append(' '.join(current_labels))\n",
    "                # Reset for next sentence\n",
    "                current_tokens = []\n",
    "                current_labels = []\n",
    "\n",
    "    # Add the last sentence if the data doesn't end with an empty line\n",
    "    if current_tokens:\n",
    "        input_texts.append(' '.join(current_tokens))\n",
    "        target_labels.append(' '.join(current_labels))\n",
    "    \n",
    "    return input_texts, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n",
      "31943 31943\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = 'Training_Data/AutoLabelledSet.txt'  # Replace with your file path\n",
    "input_texts, target_labels = preprocess_and_tokenize(file_path)\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])\n",
    "print(len(input_texts), len(target_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The geology of the hole was dominated by felsic schists and granites .', 'The mineralisation was characterised by traces of disseminated pyrite with zones of trace pyrrhotite and chalcopyrite in felsic schist .', 'The best mineralisation was intersected in felsic schists below the interpreted position of the VTEM plate model .', 'The geology of the hole was ultramafic schists overlying amphibolite and amphibolitic schists with two 1 m wide weakly sulphidic quartz veins .', 'These quartz veins were characterised by green colouration with traces of magnetite and disseminated pyrite , however contained no anomalous chemistry .']\n",
      "['O O O O O O O O B-ROCK I-ROCK O B-ROCK O', 'O O O O O O O O B-MINERAL O O O O B-MINERAL O B-MINERAL O B-ROCK I-ROCK O', 'O O O O O O B-ROCK I-ROCK O O O O O O O O O O', 'O O O O O O B-ROCK I-ROCK O B-ROCK O B-ROCK I-ROCK O O O O O O O B-ROCK I-ROCK O', 'O B-ROCK I-ROCK O O O O O O O O B-MINERAL O O B-MINERAL O O O O O O O']\n",
      "2001 2001\n",
      "['the geology of the hole was dominated by felsic schists and granites .', 'the mineralisation was characterised by traces of disseminated pyrite with zones of trace pyrrhotite and chalcopyrite in felsic schist .', 'the best mineralisation was intersected in felsic schists below the interpreted position of the vtem plate model .', 'the geology of the hole was ultramafic schists overlying amphibolite and amphibolitic schists with two 1 m wide weakly sulphidic quartz veins .', 'these quartz veins were characterised by green colouration with traces of magnetite and disseminated pyrite , however contained no anomalous chemistry .']\n"
     ]
    }
   ],
   "source": [
    "test_file = \"Training_Data/EvaluationSet.txt\"\n",
    "test_inputs, test_labels = preprocess_and_tokenize(test_file)\n",
    "print(test_inputs[:5])\n",
    "print(test_labels[:5])\n",
    "print(len(test_inputs), len(test_labels))\n",
    "for i in range(len(test_inputs)):\n",
    "    test_inputs[i] = test_inputs[i].lower()\n",
    "\n",
    "print(test_inputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Initialize lists for sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each entry in the JSON data\n",
    "    for entry in data:\n",
    "        output = entry['output']\n",
    "        label = entry['labels']\n",
    "        \n",
    "        # Join the output list into a string sentence\n",
    "        sentence = ' '.join(output).lower()\n",
    "        \n",
    "        # Join the labels list into a string\n",
    "        label_str = ' '.join(label)\n",
    "        \n",
    "        # Append to lists\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label_str)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and labels.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    targets = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure labels are the same length as inputs\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the southern portion of the forrestania greenstone belt lies 17 km to the north .\n",
      "O O O O O B-LOCATION B-STRAT I-STRAT O O O O O O O\n",
      "31943\n",
      "31943\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "train_input = [words for words in input_texts] \n",
    "train_labels = [labels for labels in target_labels]\n",
    "\n",
    "print(train_input[-1])\n",
    "print(train_labels[-1])\n",
    "print(len(train_input))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with AutoLabelledSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([31943, 256])\n",
      "Label Shape: torch.Size([31943, 256])\n",
      "Input Shape: torch.Size([2001, 256])\n",
      "Label Shape: torch.Size([2001, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/t5-base-conll03-english\", clean_up_tokenization_spaces=False)\n",
    "\n",
    "# Tokenize training and testing data\n",
    "train_data = tokenize_data(train_input, train_labels, tokenizer)\n",
    "test_data = tokenize_data(test_inputs, test_labels, tokenizer)\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NERDataset(train_data, train_data['labels'])\n",
    "test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "\n",
    "train_loader1 = DataLoader(train_dataset, batch_size=32, num_workers=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=8)\n",
    "\n",
    "# Print or return the shape of the inputs and labels\n",
    "input_shape1 = train_data['input_ids'].shape\n",
    "label_shape1 = train_data['input_ids'].shape\n",
    "\n",
    "print(f\"Input Shape: {input_shape1}\")\n",
    "print(f\"Label Shape: {label_shape1}\")\n",
    "\n",
    "# Print or return the shape of the inputs and labels\n",
    "test_input_shape = test_data['input_ids'].shape\n",
    "test_label_shape = test_data['input_ids'].shape\n",
    "\n",
    "print(f\"Input Shape: {test_input_shape}\")\n",
    "print(f\"Label Shape: {test_label_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"dbmdz/t5-base-conll03-english\").to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=15000,\n",
    "    eval_steps=15000,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Flatten the lists\n",
    "    labels = labels.flatten()\n",
    "    predictions = predictions.flatten()\n",
    "    return {\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from ./Models/6-GeoEntityNER\n"
     ]
    }
   ],
   "source": [
    "save_directory = './Models/6-GeoEntityNER'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(save_directory):\n",
    "    # Save model and tokenizer if it doesn't exist\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(f'Model and tokenizer saved to {save_directory}')\n",
    "else:\n",
    "    # Load the tokenizer and model from the saved directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "    print(f'Model and tokenizer loaded from {save_directory}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O O O O O O O O O O O O']\n",
      "Text: the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\n",
      "Prediction: O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O O O O O O O O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example geological text\n",
    "geo_texts = [\n",
    "    \"the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\",\n",
    "]\n",
    "\n",
    "# Predict geological entities\n",
    "predictions = predict_entities(geo_texts, model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(predictions)\n",
    "for text, prediction in zip(geo_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formation of the Rocky Mountains occurred around 70 Ma.\n",
      "['O O O O B-LOCATION I-LOCATION O O O B-GEO_TIME I-GEO_TIME O']\n"
     ]
    }
   ],
   "source": [
    "texts = \"The formation of the Rocky Mountains occurred around 70 Ma.\"\n",
    "\n",
    "predictions = predict_entities(texts, model, tokenizer)\n",
    "\n",
    "print(texts)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 2001 - Elapsed time: 72.75s, Estimated remaining time: 1382.90s\n",
      "200 / 2001 - Elapsed time: 178.10s, Estimated remaining time: 1603.75s\n",
      "300 / 2001 - Elapsed time: 263.39s, Estimated remaining time: 1493.43s\n",
      "400 / 2001 - Elapsed time: 345.28s, Estimated remaining time: 1381.99s\n",
      "500 / 2001 - Elapsed time: 435.71s, Estimated remaining time: 1307.99s\n",
      "600 / 2001 - Elapsed time: 532.57s, Estimated remaining time: 1243.56s\n",
      "700 / 2001 - Elapsed time: 634.76s, Estimated remaining time: 1179.75s\n",
      "800 / 2001 - Elapsed time: 722.16s, Estimated remaining time: 1084.15s\n",
      "900 / 2001 - Elapsed time: 804.73s, Estimated remaining time: 984.46s\n",
      "1000 / 2001 - Elapsed time: 892.17s, Estimated remaining time: 893.06s\n",
      "1100 / 2001 - Elapsed time: 974.85s, Estimated remaining time: 798.49s\n",
      "1200 / 2001 - Elapsed time: 1068.25s, Estimated remaining time: 713.06s\n",
      "1300 / 2001 - Elapsed time: 1149.82s, Estimated remaining time: 620.02s\n",
      "1400 / 2001 - Elapsed time: 1239.87s, Estimated remaining time: 532.26s\n",
      "1500 / 2001 - Elapsed time: 1334.24s, Estimated remaining time: 445.64s\n",
      "1600 / 2001 - Elapsed time: 1420.60s, Estimated remaining time: 356.04s\n",
      "1700 / 2001 - Elapsed time: 1511.31s, Estimated remaining time: 267.59s\n",
      "1800 / 2001 - Elapsed time: 1598.09s, Estimated remaining time: 178.45s\n",
      "1900 / 2001 - Elapsed time: 1667.45s, Estimated remaining time: 88.64s\n",
      "2000 / 2001 - Elapsed time: 1734.21s, Estimated remaining time: 0.87s\n",
      "Total evaluation time: 1735.13s\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from the saved directory\n",
    "save_directory = './Models/6-GeoEntityNER'\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "# Function to map 'B-' and 'I-' labels to their corresponding entity type\n",
    "def get_entity_type(label):\n",
    "    if label.startswith('B-') or label.startswith('I-'):\n",
    "        return label[2:]  # Remove 'B-' or 'I-' prefix\n",
    "    else:\n",
    "        return label  # Keep the label as is\n",
    "\n",
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)  # Generate output sequences\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(texts, true_labels, model, tokenizer):\n",
    "    # Get predicted texts from the model\n",
    "    size = len(texts)\n",
    "    texts = texts\n",
    "    predicted_texts = []\n",
    "    start_time = time.time()\n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_text = elapsed_time / i\n",
    "            remaining_time = avg_time_per_text * (size - i)\n",
    "            print(f\"{i} / {size} - Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "        predicted_texts.append(predict_entities(text, model, tokenizer)[0])\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total evaluation time: {total_time:.2f}s\")\n",
    "\n",
    "    return predicted_texts\n",
    "    \n",
    "# Evaluate the model\n",
    "predicted_texts = evaluate_model(test_inputs, test_labels, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 2001\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    LOCATION       0.63      0.61      0.62      1692\n",
      "     MINERAL       0.74      0.81      0.77      1403\n",
      " ORE_DEPOSIT       0.81      0.74      0.78       682\n",
      "        ROCK       0.75      0.72      0.74      2634\n",
      "       STRAT       0.83      0.72      0.77      1420\n",
      "   TIMESCALE       0.79      0.77      0.78       213\n",
      "\n",
      "   micro avg       0.74      0.72      0.73      8044\n",
      "   macro avg       0.76      0.73      0.74      8044\n",
      "weighted avg       0.74      0.72      0.73      8044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels = test_labels\n",
    "print(len(true_labels), len(predicted_texts))\n",
    "\n",
    "true_labels_flat = []\n",
    "pred_labels_flat = []\n",
    "\n",
    "for i in range(len(test_inputs)):\n",
    "    pred_tokens = predicted_texts[i].split()\n",
    "    true_tokens = true_labels[i].split()\n",
    "    \n",
    "    for j in range(len(true_tokens)):\n",
    "        # Skip 'O' labels and avoid out-of-bound errors in predictions\n",
    "        if j < len(pred_tokens) and (true_tokens[j] != 'O' or pred_tokens[j] != 'O'):\n",
    "            true_label = get_entity_type(true_tokens[j])\n",
    "            pred_label = get_entity_type(pred_tokens[j])\n",
    "\n",
    "            true_labels_flat.append(true_label)\n",
    "            pred_labels_flat.append(pred_label)\n",
    "                \n",
    "# Calculate metrics\n",
    "report = classification_report(\n",
    "    true_labels_flat, pred_labels_flat, labels=[\n",
    "        'LOCATION', 'MINERAL', 'ORE_DEPOSIT', 'ROCK', 'STRAT', 'TIMESCALE'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
