{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain_Dictionary_Path = {\"geological_timescales\": \"../Domain_Dictionary/geological_timescales.txt\",\n",
    "#                           \"locations\": \"../Domain_Dictionary/locations.txt\",\n",
    "#                           \"minerals\": \"../Domain_Dictionary/minerals.txt\",\n",
    "#                           \"ores_deposits\": \"../Domain_Dictionary/ores_deposits.txt\",\n",
    "#                           \"rocks\": \"../Domain_Dictionary/rocks.txt\",\n",
    "#                           \"stratigraphy\": \"../Domain_Dictionary/stratigraphy.txt\"}\n",
    "\n",
    "# Domain_Dictionary = {}\n",
    "\n",
    "# for domain, path in Domain_Dictionary_Path.items():\n",
    "#     with open(path, \"r\") as file:\n",
    "#         info = file.read().splitlines()\n",
    "#         if domain == \"geological_timescales\":\n",
    "#             info = [i.lstrip().split()[0] for i in info]\n",
    "#         info = [i for i in info if i.strip()] # Make sure there are no empty strings\n",
    "#         Domain_Dictionary[domain] = info\n",
    "\n",
    "#         print(info)\n",
    "#         print(len(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(file_path):\n",
    "    \"\"\"\n",
    "    Reads the file, preprocesses it, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    input_texts = []\n",
    "    target_labels = []\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Non-empty line\n",
    "            token, label = line.split()\n",
    "            current_tokens.append(token)\n",
    "            current_labels.append(label)\n",
    "        else:  # Empty line signifies end of a sentence\n",
    "            if current_tokens:\n",
    "                input_texts.append(' '.join(current_tokens))\n",
    "                target_labels.append(' '.join(current_labels))\n",
    "                # Reset for next sentence\n",
    "                current_tokens = []\n",
    "                current_labels = []\n",
    "\n",
    "    # Add the last sentence if the data doesn't end with an empty line\n",
    "    if current_tokens:\n",
    "        input_texts.append(' '.join(current_tokens))\n",
    "        target_labels.append(' '.join(current_labels))\n",
    "    \n",
    "    return input_texts, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the jubilee domain contains a complex sequence of ultramafic mafic rocks and interleaved sedimentary rocks , overlain by polymictic conglomerate .', 'extensive weathered banded quartz magnetite rocks occur throughout the project area .', 'the most common and significant metaliferrous rock types in the area are metamorphosed banded iron formations , ( bif ) and granular iron formations , ( gif ) .', 'these can be either completely oxidised medium to coarse grain hematite or more stable magnetite .', 'a major volcanic centre , defined by abundant felsic volcanics and quartz aluminosilicatechloritoid rocks ( considered to represent metamorphosed alteration assemblages ) , occurs in the central portion of this domain .']\n",
      "['O B-LOCATION O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK I-ROCK O O O B-ROCK I-ROCK O', 'O O O B-MINERAL B-MINERAL O O O O O O O', 'O O O O O B-ROCK I-ROCK O O O O O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O B-ROCK I-ROCK I-ROCK O O B-ROCK O O', 'O O O O O O O O O O B-MINERAL O O O B-MINERAL O', 'O O O O O O O O B-ROCK I-ROCK O B-MINERAL O O O O O O O O O O O O O O O O O O O O']\n",
      "31943 31943\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = 'Training_Data/AutoLabelledSet.txt'  # Replace with your file path\n",
    "input_texts, target_labels = preprocess_and_tokenize(file_path)\n",
    "print(input_texts[:5])\n",
    "print(target_labels[:5])\n",
    "print(len(input_texts), len(target_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The geology of the hole was dominated by felsic schists and granites .', 'The mineralisation was characterised by traces of disseminated pyrite with zones of trace pyrrhotite and chalcopyrite in felsic schist .', 'The best mineralisation was intersected in felsic schists below the interpreted position of the VTEM plate model .', 'The geology of the hole was ultramafic schists overlying amphibolite and amphibolitic schists with two 1 m wide weakly sulphidic quartz veins .', 'These quartz veins were characterised by green colouration with traces of magnetite and disseminated pyrite , however contained no anomalous chemistry .']\n",
      "['O O O O O O O O B-ROCK I-ROCK O B-ROCK O', 'O O O O O O O O B-MINERAL O O O O B-MINERAL O B-MINERAL O B-ROCK I-ROCK O', 'O O O O O O B-ROCK I-ROCK O O O O O O O O O O', 'O O O O O O B-ROCK I-ROCK O B-ROCK O B-ROCK I-ROCK O O O O O O O B-ROCK I-ROCK O', 'O B-ROCK I-ROCK O O O O O O O O B-MINERAL O O B-MINERAL O O O O O O O']\n",
      "2001 2001\n"
     ]
    }
   ],
   "source": [
    "test_file = \"Training_Data/EvaluationSet.txt\"\n",
    "test_inputs, test_labels = preprocess_and_tokenize(test_file)\n",
    "print(test_inputs[:5])\n",
    "print(test_labels[:5])\n",
    "print(len(test_inputs), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Initialize lists for sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each entry in the JSON data\n",
    "    for entry in data:\n",
    "        output = entry['output']\n",
    "        label = entry['labels']\n",
    "        \n",
    "        # Join the output list into a string sentence\n",
    "        sentence = ' '.join(output).lower()\n",
    "        \n",
    "        # Join the labels list into a string\n",
    "        label_str = ' '.join(label)\n",
    "        \n",
    "        # Append to lists\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label_str)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aalenian', 'abereiddian', 'acadian', 'actonian', 'adelaidean']\n",
      "['B-TIMESCALE', 'B-TIMESCALE', 'B-TIMESCALE', 'B-TIMESCALE', 'B-TIMESCALE']\n",
      "16920 16920\n",
      "['cookernup', 'rayite', 'cobb formation', 'mapingian', 'grave dam grit']\n",
      "['B-LOCATION', 'B-MINERAL', 'B-STRAT I-STRAT', 'B-TIMESCALE', 'B-STRAT I-STRAT I-STRAT']\n",
      "67680 67680\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_name = \"Training_Data/DomainDictionary.json\"  # Replace with your file path\n",
    "dictionary_words, dictionary_labels = preprocess_and_tokenize_json(file_name)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "print(dictionary_words[:5])\n",
    "print(dictionary_labels[:5])\n",
    "print(len(dictionary_words), len(dictionary_labels))\n",
    "\n",
    "combined = list(zip(dictionary_words * 4, dictionary_labels * 4))\n",
    "\n",
    "random.shuffle(combined)\n",
    "\n",
    "dictionary_words , dictionary_labels = zip(*combined)\n",
    "dictionary_words = list(dictionary_words)\n",
    "dictionary_labels = list(dictionary_labels)\n",
    "\n",
    "print(dictionary_words[:5])\n",
    "print(dictionary_labels[:5])\n",
    "print(len(dictionary_words), len(dictionary_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3998\n"
     ]
    }
   ],
   "source": [
    "dd_words = []\n",
    "dd_labels = []\n",
    "\n",
    "sentence = \"\"\n",
    "labels = \"\"\n",
    "num = 60\n",
    "random.seed(42)\n",
    "for i in range(len(dictionary_words)):\n",
    "    word = dictionary_words[i]\n",
    "    label = dictionary_labels[i]\n",
    "    if len(sentence.split(\" \")) + len(word.split(\" \")) > num:\n",
    "        dd_words.append(sentence)\n",
    "        dd_labels.append(labels)\n",
    "        sentence = \"\"\n",
    "        labels = \"\"\n",
    "        sentence += word\n",
    "        labels += label\n",
    "    else:\n",
    "        if sentence == \"\":\n",
    "            sentence += word\n",
    "            labels += label\n",
    "        else:\n",
    "            sentence += \" , \" + word\n",
    "            labels += \" O \" + label\n",
    "        num = random.randint(40, 60)\n",
    "\n",
    "print(len(dd_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cookernup , rayite , cobb formation , mapingian , grave dam grit , inkamulla granodiorite , fowlerite , davidsmithite , green head , spaltiite , timber creek formation , freshwater limestone , munyi member , volcaniclastic conglomerate', 'mount caernarvon greywacke member , wadjemup formation , twin bonanza porphyry , colimaite , vitimite , bb19 , georgeite , sinter , mudnawatana tonalite , natrolite , parmelia formation , yarragadee formation , kalinjala mylonite , clinoptilolite k , mount andrew migmatite , tertiary , horn valley siltstone', 'childrenite , vincent member , tashelgite , meta ultramafic intrusive rock , monohydrocalcite , sterryite , crossroads granodiorite , bafertisite , magnesiocarbonatite , daly river group , yilgarn star , quartzofeldspathic schist , yangzhumingite , wirraway formation , pelsart limestone , mougooderra formation , paulingite k', 'pottsite , polylithionite , hechtsbergite , dougalls tonalite , riebeckite , mesoproterozoic granites 76633 , ankerite sparstone , ardaite , newhaven shale member , pantapinna sandstone , nickelbischofite , cadmium ochre , laurite , springvale suite , jim jim suite , rupelian , silhydrite , carynginia shale ', 'minnie creek granodiorite , ashcroftine y , bond springs gneiss , bussenite , metamorphic protolith fault rock , arenigian , petewilliamsite , defernite , fitzroy volcanics , florensovite , windidda member , riversdale formation , red lead ore , seale sandstone , eucla basin']\n",
      "['B-LOCATION O B-MINERAL O B-STRAT I-STRAT O B-TIMESCALE O B-STRAT I-STRAT I-STRAT O B-STRAT I-STRAT O B-MINERAL O B-MINERAL O B-LOCATION I-LOCATION O B-MINERAL O B-STRAT I-STRAT I-STRAT O B-STRAT I-STRAT O B-STRAT I-STRAT O B-ROCK I-ROCK', 'B-STRAT I-STRAT I-STRAT I-STRAT O B-STRAT I-STRAT O B-STRAT I-STRAT I-STRAT O B-MINERAL O B-MINERAL O B-STRAT O B-MINERAL O B-ROCK O B-STRAT I-STRAT O B-MINERAL O B-STRAT I-STRAT O B-STRAT I-STRAT O B-STRAT I-STRAT O B-MINERAL I-MINERAL O B-STRAT I-STRAT I-STRAT O B-TIMESCALE O B-STRAT I-STRAT I-STRAT', 'B-MINERAL O B-STRAT I-STRAT O B-MINERAL O B-ROCK I-ROCK I-ROCK I-ROCK O B-MINERAL O B-MINERAL O B-STRAT I-STRAT O B-MINERAL O B-ROCK O B-STRAT I-STRAT I-STRAT O B-LOCATION I-LOCATION O B-ROCK I-ROCK O B-MINERAL O B-STRAT I-STRAT O B-STRAT I-STRAT O B-STRAT I-STRAT O B-MINERAL I-MINERAL', 'B-MINERAL O B-MINERAL O B-MINERAL O B-STRAT I-STRAT O B-MINERAL O B-STRAT I-STRAT I-STRAT O B-ROCK I-ROCK O B-MINERAL O B-STRAT I-STRAT I-STRAT O B-STRAT I-STRAT O B-MINERAL O B-MINERAL I-MINERAL O B-MINERAL O B-STRAT I-STRAT O B-STRAT I-STRAT I-STRAT O B-TIMESCALE O B-MINERAL O B-STRAT I-STRAT I-STRAT', 'B-STRAT I-STRAT I-STRAT O B-MINERAL I-MINERAL O B-STRAT I-STRAT I-STRAT O B-MINERAL O B-ROCK I-ROCK I-ROCK I-ROCK O B-TIMESCALE O B-MINERAL O B-MINERAL O B-STRAT I-STRAT O B-MINERAL O B-STRAT I-STRAT O B-STRAT I-STRAT O B-ORE_DEPOSIT I-ORE_DEPOSIT I-ORE_DEPOSIT O B-STRAT I-STRAT O B-LOCATION I-LOCATION']\n",
      "3998 3998\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(dd_words[:5])\n",
    "print(dd_labels[:5])\n",
    "print(len(dd_words), len(dd_labels))\n",
    "\n",
    "print(len(dd_words[0].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "#     input_texts, target_labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# print(f\"Number of training samples: {len(train_texts)}\")\n",
    "# print(f\"Number of testing samples: {len(test_texts)}\")\n",
    "\n",
    "# print(train_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and labels.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    targets = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure labels are the same length as inputs\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandstoneother siliciclastic rock , buckleboo granite , dollaseite ce , bokite , areyonga formation , indium , fülöppite , bottle creek gold mine , bottinoite , wattevilleite , dalgaranga dolerite , grischunite , gravelly mud , dravertite , rouse creek arenite , liddle formation , condenser dolerite , ellis formation , fluorbritholite y\n",
      "B-ROCK I-ROCK I-ROCK O B-STRAT I-STRAT O B-MINERAL I-MINERAL O B-MINERAL O B-STRAT I-STRAT O B-MINERAL O B-MINERAL O B-LOCATION I-LOCATION I-LOCATION I-LOCATION O B-MINERAL O B-MINERAL O B-STRAT I-STRAT O B-MINERAL O B-ROCK I-ROCK O B-MINERAL O B-STRAT I-STRAT I-STRAT O B-STRAT I-STRAT O B-STRAT I-STRAT O B-STRAT I-STRAT O B-MINERAL I-MINERAL\n",
      "3998\n",
      "3998\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "train_input1 = [words for words in input_texts] \n",
    "# train_input.extend([words for words in geotime_text])\n",
    "# train_input.extend([words for words in dd_words])\n",
    "\n",
    "train_labels1 = [labels for labels in target_labels]\n",
    "# train_labels.extend([labels for labels in geotime_labels])\n",
    "# train_labels.extend([labels for labels in dd_labels])\n",
    "\n",
    "train_input1 = [words for words in dd_words]\n",
    "train_labels1 = [labels for labels in dd_labels]\n",
    "\n",
    "# print(train_input[-1])\n",
    "\n",
    "\n",
    "print(train_input1[-1])\n",
    "\n",
    "print(train_labels1[-1])\n",
    "\n",
    "print(len(train_input1))\n",
    "print(len(train_labels1))\n",
    "# print(len(train_input))\n",
    "# print(len(train_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with AutoLabelledSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([3998, 256])\n",
      "Label Shape: torch.Size([3998, 256])\n",
      "Input Shape: torch.Size([2001, 256])\n",
      "Label Shape: torch.Size([2001, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/t5-base-conll03-english\", clean_up_tokenization_spaces=False)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "\n",
    "# Tokenize training and testing data\n",
    "train_data1 = tokenize_data(train_input1, train_labels1, tokenizer)\n",
    "test_data = tokenize_data(test_inputs, test_labels, tokenizer)\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset1 = NERDataset(train_data1, train_data1['labels'])\n",
    "test_dataset = NERDataset(test_data, test_data['labels'])\n",
    "\n",
    "\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=32, num_workers=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=8)\n",
    "\n",
    "# Print or return the shape of the inputs and labels\n",
    "input_shape1 = train_data1['input_ids'].shape\n",
    "label_shape1 = train_data1['input_ids'].shape\n",
    "\n",
    "print(f\"Input Shape: {input_shape1}\")\n",
    "print(f\"Label Shape: {label_shape1}\")\n",
    "\n",
    "# Print or return the shape of the inputs and labels\n",
    "test_input_shape = test_data['input_ids'].shape\n",
    "test_label_shape = test_data['input_ids'].shape\n",
    "\n",
    "print(f\"Input Shape: {test_input_shape}\")\n",
    "print(f\"Label Shape: {test_label_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"dbmdz/t5-base-conll03-english\").to(device)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\").to(device)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=15000,\n",
    "    eval_steps=15000,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Flatten the lists\n",
    "    labels = labels.flatten()\n",
    "    predictions = predictions.flatten()\n",
    "    return {\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset1,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yulleroo formation , yulleroo sub-basin , western australia .']\n",
      "Text: yulleroo formation , yulleroo sub-basin , western australia .\n",
      "Prediction: yulleroo formation , yulleroo sub-basin , western australia .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example geological text\n",
    "geo_texts = [\n",
    "    \"the jubilee domain contains a complex sequence of ultramafic mafic rocks and more evolved rocks, due to magma mixing and fractional crystallization .\",\n",
    "]\n",
    "\n",
    "geo_texts = [\"yulleroo formation , yulleroo sub-basin , western australia .\",]\n",
    "\n",
    "# Predict geological entities\n",
    "predictions = predict_entities(geo_texts, model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(predictions)\n",
    "for text, prediction in zip(geo_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./Models/6-GeoEntityNER')  # Custom directory for saving\n",
    "# tokenizer.save_pretrained('./Models/6-GeoEntityNER')  # Save tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    save_directory = './Models/6-GeoEntityNER'\n",
    "    # Load the tokenizer and model from the saved directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-STRAT I-STRAT O B-STRAT I-STRAT O B-LOCATION I-LOCATION O']\n",
      "Text: yulleroo formation , yulleroo sub-basin , western australia .\n",
      "Prediction: B-STRAT I-STRAT O B-STRAT I-STRAT O B-LOCATION I-LOCATION O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict geological entities\n",
    "predictions = predict_entities(geo_texts, model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(predictions)\n",
    "for text, prediction in zip(geo_texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formation of the Rocky Mountains occurred around 70 Ma.\n",
      "['O O O O B-LOCATION I-LOCATION O O O B-GEO_TIME I-GEO_TIME O']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "texts = \"The formation of the Rocky Mountains occurred around 70 Ma.\"\n",
    "\n",
    "predictions = predict_entities(texts, model, tokenizer)\n",
    "\n",
    "print(texts)\n",
    "print(predictions)\n",
    "\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6507\n",
      "F1-score (Micro): 0.6507\n",
      "F1-score (Macro): 0.6750\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    LOCATION       0.58      0.85      0.69      1692\n",
      "     MINERAL       0.78      0.86      0.82      1403\n",
      " ORE_DEPOSIT       0.88      0.78      0.83       682\n",
      "        ROCK       0.82      0.74      0.78      2631\n",
      "       STRAT       0.77      0.76      0.77      1417\n",
      "   TIMESCALE       0.84      0.86      0.85       213\n",
      "\n",
      "   micro avg       0.74      0.80      0.77      8038\n",
      "   macro avg       0.78      0.81      0.79      8038\n",
      "weighted avg       0.76      0.80      0.77      8038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Load the tokenizer and model from the saved directory\n",
    "# save_directory = \"../Models/saved_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "# # Load the tokenizer and model from the saved directory\n",
    "# save_directory = \"./Models/double_trained_model\"\n",
    "# new_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# new_model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "# Function to map 'B-' and 'I-' labels to their corresponding entity type\n",
    "def get_entity_type(label):\n",
    "    if label.startswith('B-') or label.startswith('I-'):\n",
    "        return label[2:]  # Remove 'B-' or 'I-' prefix\n",
    "    else:\n",
    "        return label  # Keep the label as is\n",
    "\n",
    "def tokenize_input(texts, tokenizer, max_length=256):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def predict_entities(texts, model, tokenizer):\n",
    "    inputs = tokenize_input(texts, tokenizer)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)  # Generate output sequences\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(texts, true_labels, model, tokenizer):\n",
    "    # Get predicted texts from the model\n",
    "    texts = texts\n",
    "    predicted_texts = []\n",
    "    for text in texts:\n",
    "        predicted_texts.append(predict_entities(text, model, tokenizer)[0])\n",
    "    \n",
    "    true_labels_flat = []\n",
    "    pred_labels_flat = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        pred_tokens = predicted_texts[i].split()\n",
    "        true_tokens = true_labels[i].split()\n",
    "        \n",
    "        for j in range(len(true_tokens)):\n",
    "            # Skip 'O' labels and avoid out-of-bound errors in predictions\n",
    "            if j < len(pred_tokens) and (true_tokens[j] != 'O' or pred_tokens[j] != 'O'):\n",
    "                true_label = get_entity_type(true_tokens[j])\n",
    "                pred_label = get_entity_type(pred_tokens[j])\n",
    "\n",
    "                true_labels_flat.append(true_label)\n",
    "                pred_labels_flat.append(pred_label)\n",
    "                    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "    f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "    f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "    \n",
    "    report = classification_report(\n",
    "        true_labels_flat, pred_labels_flat, labels=[\n",
    "            'LOCATION', 'MINERAL', 'ORE_DEPOSIT', 'ROCK', 'STRAT', 'TIMESCALE'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the model on the parsed test data\n",
    "evaluate_model(test_inputs, test_labels, model, tokenizer)\n",
    "# evaluate_model(test_inputs, test_labels, old_model, old_tokenizer)\n",
    "# evaluate_model(test_inputs, test_labels, new_model, new_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Accuracy: 0.6507\n",
    "F1-score (Micro): 0.6507\n",
    "F1-score (Macro): 0.6750\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    LOCATION       0.58      0.85      0.69      1692\n",
    "     MINERAL       0.78      0.86      0.82      1403\n",
    " ORE_DEPOSIT       0.88      0.78      0.83       682\n",
    "        ROCK       0.82      0.74      0.78      2631\n",
    "       STRAT       0.77      0.76      0.77      1417\n",
    "   TIMESCALE       0.84      0.86      0.85       213\n",
    "\n",
    "   micro avg       0.74      0.80      0.77      8038\n",
    "   macro avg       0.78      0.81      0.79      8038\n",
    "weighted avg       0.76      0.80      0.77      8038\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
