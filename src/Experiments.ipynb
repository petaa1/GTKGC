{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: NER Flair Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = None\n",
    "with open (\"NER/SyntheticData/data.json\", \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df, filepath):\n",
    "\n",
    "    with open(filepath, \"w\") as file:\n",
    "        for text, annotations in df:\n",
    "            for token, label in annotations:\n",
    "                token = preprocess_word(token)\n",
    "                if token != \" \":\n",
    "                    file.write(f\"{token} {label}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "\n",
    "create_data(data, \"NER/SyntheticData/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "\n",
    "def train_flair_ner(data_version):\n",
    "    columns = {0: \"text\", 1: \"ner\"}\n",
    "\n",
    "    corpus = ColumnCorpus(\".\", columns, train_file=\"NER/Training_Data/AutoLabelled.txt\")\n",
    "\n",
    "    label_dict = corpus.make_label_dictionary(label_type='ner')\n",
    "\n",
    "    # Initialise embeddings\n",
    "    embedding_types : List[TokenEmbeddings] = [\n",
    "            WordEmbeddings('glove'),\n",
    "            # other embeddings\n",
    "            ]\n",
    "    embeddings : StackedEmbeddings = StackedEmbeddings(\n",
    "                                    embeddings=embedding_types)\n",
    "\n",
    "    # Initialise sequence tagger\n",
    "    tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=label_dict,\n",
    "                                        tag_type='ner',\n",
    "                                        use_crf=True)\n",
    "\n",
    "    # Initialise trainer - Training\n",
    "    trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "    trainer.train(f'flair_models/{data_version}',\n",
    "                learning_rate=0.1,\n",
    "                mini_batch_size=64,\n",
    "                max_epochs=5)\n",
    "    \n",
    "# train_flair_ner('joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "def apply_flair_ner(model_path, document: str):\n",
    "    # Load the trained model\n",
    "    tagger = SequenceTagger.load(model_path)\n",
    "    \n",
    "    # Create a Sentence object\n",
    "    sentence = Sentence(document)\n",
    "    \n",
    "    # Predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    print(sentence)\n",
    "    print(\"Tagged sentence:\", sentence.to_tagged_string())\n",
    "    \n",
    "    # Extract and print the entities\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        print(f'Entity: {entity.text}, Type: {entity.tag}, Confidence: {entity.score}')\n",
    "     # Create a mapping of token index to NER tag\n",
    "    token_tags = {i: 'O' for i in range(len(sentence))}\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        for token in entity:\n",
    "            token_tags[token.idx - 1] = entity.tag  # Adjusting for zero-based index\n",
    "    \n",
    "    # Print each word with its NER tag\n",
    "    print(\"\\nDetailed word information:\")\n",
    "    for i, token in enumerate(sentence):\n",
    "        ner_tag = token_tags[i]\n",
    "        print(f\"{token.text} {ner_tag}\")\n",
    "    \n",
    "# Example usage\n",
    "document = ['midwest', 'corporation', 'limit', 'annual', 'report', 'e09', '1004', 'september', '2006', 'page', '2', 'of', '21', 'september', '2006', '1', 'bibliographic', 'datum', 'sheet', 'report', 'title', 'midwest', 'corporation', 'limit', 'new', 'forest', 'project', 'annual', 'report', 'for', 'e09', '1004', '26', 'september', '2005', '25', 'september', '2006', 'david', 'broomfield', '20th', 'october', '2006', 'prospect', 'name', 'new', 'forest', 'project', 'tenement', 'number', 'mt', 'aubrey', 'e09', '1004', 'tenement', 'holder', 'midwest', 'corporation', 'limit', 'commodity', 'iron', 'ore', 'goldfield', 'mineral', 'field', 'south', 'west', 'mineral', 'field', '70', 'gascoyne', 'gold', 'field', '09', 'tectonic', 'unit', 'yilgarn', 'craton', 'moyagee', 'formation', 'lithologic', 'unit', 'band', 'iron', 'formation', 'massive', 'haematite', 'chert', 'quartzite', 'meta', 'sedimentary', 'rock', 'mafic', 'and', 'ultramafic', 'rock', 'laterite', '1', '250', '000', 'sheet', 'murgoo', 'sg50', '14', 'keyword', 'iron', 'ore', 'haematite', 'band', 'iron', 'formation', 'geological', 'mapping', 'geochemical', 'sample', 'airborne', 'geophysical', 'survey']\n",
    "apply_flair_ner('flair_models/joined/final-model.pt', document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Rule_based Labelling\n",
    "Label all tokens\n",
    "- with Domain Dictionary vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyahocorasick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "\n",
    "num = 0\n",
    "\n",
    "# Function to create Aho-Corasick automaton for keywords\n",
    "def create_automaton(keywords):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for idx, keyword in enumerate(keywords):\n",
    "        A.add_word(re.sub(r'[^\\w\\s]', ' ', keyword.lower()), (idx, keyword))\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "# Function to extract domain specific keywords\n",
    "def extract_and_label_domain_keywords(text, automaton, num):\n",
    "    keywords = []\n",
    "    text = \" \".join(text)\n",
    "    for end_index, (idx, keyword) in automaton.iter(text):\n",
    "        keyword_tokens = keyword.split()\n",
    "        phrase_length = len(keyword_tokens)\n",
    "        start_char_index = end_index - len(keyword) + 1\n",
    "        \n",
    "        # Find the token index where the phrase starts\n",
    "        start_token_index = len(text[:start_char_index].split())\n",
    "\n",
    "        # print(keyword_tokens)\n",
    "        # print(phrase_length) \n",
    "        # print(start_char_index)\n",
    "        # print(start_token_index)\n",
    "        # print(f\"Keyword: {keyword} found at index {end_index - len(keyword) + 1}\")\n",
    "        \n",
    "        keywords.append(keyword)\n",
    "        num += 1\n",
    "    return keywords, num\n",
    "\n",
    "# Extract all domain specific keywords\n",
    "domains = [\"minerals\", \"rocks\", \"stratigraphy\", \"ores_deposits\", \"locations\", \"geological_timescales\"]\n",
    "automatons_dictionary = {d: create_automaton(Domain_Dictionary[d]) for d in domains}\n",
    "\n",
    "for d in domains:\n",
    "    for i in range(len(output)):\n",
    "        print(f\"Page {i+1}:\")\n",
    "        keywords, num = extract_and_label_domain_keywords(output[i], automatons_dictionary[d], num)\n",
    "        print(keywords)\n",
    "\n",
    "print(\"num = \", num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Synthetic Data\n",
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "# Sentence templates for each domain\n",
    "templates = {\n",
    "    \"minerals\": [\"{entity} is a common mineral.\", \"The composition of {entity} is well-studied.\"],\n",
    "    \"rocks\": [\"{entity} is a type of rock.\", \"{entity} is often found in mountain regions.\"],\n",
    "    \"stratigraphy\": [\"{entity} provides important geological information.\", \"The {entity} is used to identify rock layers.\"],\n",
    "    \"ores_deposits\": [\"{entity} is mined for its value.\", \"The {entity} is an important resource.\"],\n",
    "    \"locations\": [\"The {entity} is located at a high altitude.\", \"{entity} is a significant geographic feature.\"],\n",
    "    \"geological_timescales\": [\"The {entity} is a major time period.\", \"{entity} marks a significant era in history.\"]\n",
    "}\n",
    "\n",
    "# Generate synthetic training data\n",
    "def generate_synthetic_data(domain_dict, templates, num_sentences=10000):\n",
    "    synthetic_data = []\n",
    "    for _ in range(num_sentences):\n",
    "        # Randomly select a domain and a word from that domain\n",
    "        for domain, words in domain_dict.items():\n",
    "            word = random.choice(words)\n",
    "            template = random.choice(templates[domain])\n",
    "            sentence = template.format(entity=word)\n",
    "\n",
    "            # Combine with another domain for multiple entity types\n",
    "            domain2 = random.choice([d for d in domain_dict.keys() if d != domain])\n",
    "            word2 = random.choice(domain_dict[domain2])\n",
    "            template2 = random.choice(templates[domain2])\n",
    "            sentence2 = template2.format(entity=word2)\n",
    "            # Combine the two sentences\n",
    "            combined_sentence = f\"{sentence} And, {sentence2}\"\n",
    "\n",
    "            token_labels = []\n",
    "           \n",
    "            # Tokenize the sentence and label the tokens\n",
    "            doc = nlp(combined_sentence)\n",
    "\n",
    "            word_split = word.split()\n",
    "            word_split2 = word2.split()\n",
    "            current_words = word_split\n",
    "            current_domain = domain\n",
    "\n",
    "            for token in doc:\n",
    "                if token.text == current_words[0]: \n",
    "                    label = \"B-\" + current_domain.upper()\n",
    "                elif len(current_words) > 1 and token.text in current_words[1:]:\n",
    "                    label = \"I-\" + current_domain.upper()\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                    if token.text == \"And\":\n",
    "                        current_words = word_split2\n",
    "                        current_domain = domain2\n",
    "                token_labels.append((token.text, label))\n",
    "\n",
    "            synthetic_data.append((combined_sentence, token_labels))\n",
    "    return synthetic_data\n",
    "\n",
    "# synthetic_data = generate_synthetic_data(Domain_Dictionary, templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Load the spaCy model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "domains = [\"minerals\", \"rocks\", \"stratigraphy\", \"ores_deposits\", \"locations\", \"geological_timescales\"]\n",
    "\n",
    "# Generate synthetic training data\n",
    "def generate_synthetic_data_with_openai(domain_dict, in_file, out_file, num_sentences=10000):\n",
    "    synthetic_data = []\n",
    "\n",
    "    templates = []\n",
    "    with open(in_file, 'r') as file:\n",
    "        for line in file:\n",
    "            templates.append(line.strip())\n",
    "\n",
    "    temp_num = 0\n",
    "    with open(out_file, 'w') as file:\n",
    "        for i in range(num_sentences):\n",
    "            template = templates[temp_num]\n",
    "            if temp_num == len(templates) - 1:\n",
    "                temp_num = 0\n",
    "            else:\n",
    "                temp_num += 1\n",
    "\n",
    "            # template = template.split(\" \")\n",
    "            split_template = re.findall(r'\\{.*?\\}|\\w+|[.,]', template)\n",
    "\n",
    "            for word in split_template:\n",
    "                entity = re.search(r'\\{(.*?)\\}', word)\n",
    "                if (entity):\n",
    "                    domain = entity.group(1)\n",
    "                    rand_word = random.choice(domain_dict[domain])\n",
    "                    \n",
    "                    rand_word = rand_word.split()\n",
    "                    for w in rand_word:\n",
    "                        if w == rand_word[0]:\n",
    "                            label = \"B-\" + domain.upper()\n",
    "                            file.write(w + \" \" + label + \"\\n\")\n",
    "                        else:\n",
    "                            label = \"I-\" + domain.upper()\n",
    "                            file.write(w + \" \" + label+ \"\\n\")\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                    file.write(word + \" \" + label + \"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    return synthetic_data\n",
    "in_file = Path(\"NER\") / \"DataExamples.txt\"\n",
    "out_file = Path(\"NER\") / \"TrainingData.txt\"\n",
    "synthetic_data = generate_synthetic_data_with_openai(Domain_Dictionary, in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, annotations in synthetic_data[:5]:\n",
    "    print(text, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to JSON file\n",
    "# with open(\"NER/data.json\", \"w\") as json_file:\n",
    "#     json.dump(synthetic_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"NER/data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for text, annotations in data[:5]:\n",
    "        print(text, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Temporal Time Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datefinder\n",
    "\n",
    "\n",
    "# Sample text from geological surveys\n",
    "text = \"\"\"\n",
    "This report has been prepared as an investigation of the Mt Aubrey tenement, as part of Midwest’s New\n",
    "Forest project in the Murchison Region of Western Australia. The report is presented as an Annual Report\n",
    "to be submitted to the Department of Industry and Resources as part of the conditions of the granting of\n",
    "E09/1004 and covers the period from the 26 September 2005 to the 25 September 2006.\n",
    "\"\"\"\n",
    "\n",
    "# test not work well when text such as \"E09/1004\" occurs and the library mistakes it for a date\n",
    "matches = datefinder.find_dates(text)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
